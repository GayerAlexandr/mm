# python ./util.py
29286 words found in training set. Truncated to vocabulary size 29000.
Dictionary saved to file /Users/may/Tools/cnn-ld-tf/data/mr500/vocab. Max sentence length in data is 855.
Split dataset into train/test set: 900 for training, 100 for evaluation.
Reading pretrained word vectors from file ...
Generated embeddings with shape (29000, 300)

# python ./train.py --batch_size=5 --num_classes=2 --vocab_size=27000 --sent_len=800 --use_pretrain=True
--data_dir=./data/mr500 --num_epoch=50 --optimizer='adagrad' --log_step=10 --summary_step=50 --checkpoint_step=50
Step 3400: train_loss = 0.106503, train_accuracy = 0.991
Step 3400: dev_loss = 0.407296, dev_accuracy = 0.830

2016-08-28 02:46:26.734782: step 3410/9000 (epoch 19/50), acc = 1.00, loss = 0.13 (11.5 examples/sec; 0.434 sec/batch), lr: 0.010000
2016-08-28 02:46:32.599949: step 3420/9000 (epoch 19/50), acc = 1.00, loss = 0.05 (13.3 examples/sec; 0.375 sec/batch), lr: 0.010000
2016-08-28 02:46:39.573936: step 3430/9000 (epoch 20/50), acc = 1.00, loss = 0.18 (10.1 examples/sec; 0.494 sec/batch), lr: 0.010000
2016-08-28 02:46:46.537440: step 3440/9000 (epoch 20/50), acc = 1.00, loss = 0.11 (13.4 examples/sec; 0.374 sec/batch), lr: 0.010000
2016-08-28 02:46:52.526301: step 3450/9000 (epoch 20/50), acc = 1.00, loss = 0.18 (13.8 examples/sec; 0.363 sec/batch), lr: 0.010000

Step 3450: train_loss = 0.116515, train_accuracy = 0.987
Step 3450: dev_loss = 0.441663, dev_accuracy = 0.810

2016-08-28 02:47:07.832094: step 3460/9000 (epoch 20/50), acc = 1.00, loss = 0.12 (10.3 examples/sec; 0.484 sec/batch), lr: 0.010000
2016-08-28 02:47:15.602231: step 3470/9000 (epoch 20/50), acc = 1.00, loss = 0.05 (11.3 examples/sec; 0.441 sec/batch), lr: 0.010000
2016-08-28 02:47:22.221416: step 3480/9000 (epoch 20/50), acc = 1.00, loss = 0.03 (11.4 examples/sec; 0.440 sec/batch), lr: 0.010000
2016-08-28 02:47:29.309446: step 3490/9000 (epoch 20/50), acc = 1.00, loss = 0.10 (9.1 examples/sec; 0.552 sec/batch), lr: 0.010000
2016-08-28 02:47:35.808697: step 3500/9000 (epoch 20/50), acc = 1.00, loss = 0.02 (11.7 examples/sec; 0.428 sec/batch), lr: 0.010000

Step 3500: train_loss = 0.105931, train_accuracy = 0.988
Step 3500: dev_loss = 0.406511, dev_accuracy = 0.830

2016-08-28 02:47:51.624952: step 3510/9000 (epoch 20/50), acc = 1.00, loss = 0.06 (13.8 examples/sec; 0.362 sec/batch), lr: 0.010000
2016-08-28 02:47:57.789438: step 3520/9000 (epoch 20/50), acc = 1.00, loss = 0.12 (13.3 examples/sec; 0.376 sec/batch), lr: 0.010000
2016-08-28 02:48:03.857187: step 3530/9000 (epoch 20/50), acc = 1.00, loss = 0.05 (13.6 examples/sec; 0.368 sec/batch), lr: 0.010000
2016-08-28 02:48:09.800014: step 3540/9000 (epoch 20/50), acc = 1.00, loss = 0.15 (13.8 examples/sec; 0.362 sec/batch), lr: 0.010000
2016-08-28 02:48:15.924823: step 3550/9000 (epoch 20/50), acc = 1.00, loss = 0.05 (13.6 examples/sec; 0.367 sec/batch), lr: 0.010000

Step 3550: train_loss = 0.099222, train_accuracy = 0.991
Step 3550: dev_loss = 0.400379, dev_accuracy = 0.810

2016-08-28 02:48:31.216612: step 3560/9000 (epoch 20/50), acc = 1.00, loss = 0.09 (14.1 examples/sec; 0.356 sec/batch), lr: 0.010000
2016-08-28 02:48:37.316081: step 3570/9000 (epoch 20/50), acc = 1.00, loss = 0.03 (13.3 examples/sec; 0.376 sec/batch), lr: 0.010000
2016-08-28 02:48:43.290515: step 3580/9000 (epoch 20/50), acc = 1.00, loss = 0.11 (13.0 examples/sec; 0.384 sec/batch), lr: 0.010000
2016-08-28 02:48:49.239942: step 3590/9000 (epoch 20/50), acc = 1.00, loss = 0.12 (12.3 examples/sec; 0.407 sec/batch), lr: 0.010000
2016-08-28 02:48:55.190410: step 3600/9000 (epoch 20/50), acc = 1.00, loss = 0.06 (12.1 examples/sec; 0.413 sec/batch), lr: 0.010000

Step 3600: train_loss = 0.100739, train_accuracy = 0.992
Step 3600: dev_loss = 0.400934, dev_accuracy = 0.820

2016-08-28 02:49:11.021311: step 3610/9000 (epoch 21/50), acc = 1.00, loss = 0.10 (12.7 examples/sec; 0.393 sec/batch), lr: 0.010000
2016-08-28 02:49:17.717006: step 3620/9000 (epoch 21/50), acc = 1.00, loss = 0.16 (13.2 examples/sec; 0.378 sec/batch), lr: 0.010000
2016-08-28 02:49:24.436980: step 3630/9000 (epoch 21/50), acc = 1.00, loss = 0.11 (10.2 examples/sec; 0.492 sec/batch), lr: 0.010000
2016-08-28 02:49:31.247807: step 3640/9000 (epoch 21/50), acc = 1.00, loss = 0.08 (11.1 examples/sec; 0.449 sec/batch), lr: 0.010000
2016-08-28 02:49:38.313267: step 3650/9000 (epoch 21/50), acc = 1.00, loss = 0.09 (11.2 examples/sec; 0.446 sec/batch), lr: 0.010000

Step 3650: train_loss = 0.080372, train_accuracy = 1.000
Step 3650: dev_loss = 0.442338, dev_accuracy = 0.790

2016-08-28 02:49:55.104394: step 3660/9000 (epoch 21/50), acc = 1.00, loss = 0.02 (12.5 examples/sec; 0.401 sec/batch), lr: 0.010000
2016-08-28 02:50:02.361302: step 3670/9000 (epoch 21/50), acc = 1.00, loss = 0.04 (9.9 examples/sec; 0.503 sec/batch), lr: 0.010000
2016-08-28 02:50:08.705312: step 3680/9000 (epoch 21/50), acc = 1.00, loss = 0.04 (12.0 examples/sec; 0.416 sec/batch), lr: 0.010000
2016-08-28 02:50:15.347326: step 3690/9000 (epoch 21/50), acc = 1.00, loss = 0.13 (13.7 examples/sec; 0.366 sec/batch), lr: 0.010000
2016-08-28 02:50:21.220395: step 3700/9000 (epoch 21/50), acc = 1.00, loss = 0.08 (13.0 examples/sec; 0.385 sec/batch), lr: 0.010000

Step 3700: train_loss = 0.087463, train_accuracy = 0.990
Step 3700: dev_loss = 0.425731, dev_accuracy = 0.820

2016-08-28 02:50:36.310644: step 3710/9000 (epoch 21/50), acc = 1.00, loss = 0.08 (13.8 examples/sec; 0.362 sec/batch), lr: 0.010000
2016-08-28 02:50:42.841335: step 3720/9000 (epoch 21/50), acc = 1.00, loss = 0.13 (12.2 examples/sec; 0.410 sec/batch), lr: 0.010000
2016-08-28 02:50:49.600504: step 3730/9000 (epoch 21/50), acc = 1.00, loss = 0.07 (12.8 examples/sec; 0.391 sec/batch), lr: 0.010000
2016-08-28 02:50:56.330025: step 3740/9000 (epoch 21/50), acc = 1.00, loss = 0.02 (12.0 examples/sec; 0.416 sec/batch), lr: 0.010000
2016-08-28 02:51:02.787709: step 3750/9000 (epoch 21/50), acc = 1.00, loss = 0.06 (13.7 examples/sec; 0.364 sec/batch), lr: 0.010000

Step 3750: train_loss = 0.089774, train_accuracy = 0.989
Step 3750: dev_loss = 0.403849, dev_accuracy = 0.820

2016-08-28 02:51:19.690693: step 3760/9000 (epoch 21/50), acc = 1.00, loss = 0.08 (13.4 examples/sec; 0.374 sec/batch), lr: 0.010000
2016-08-28 02:51:25.593810: step 3770/9000 (epoch 21/50), acc = 1.00, loss = 0.04 (13.3 examples/sec; 0.377 sec/batch), lr: 0.010000
2016-08-28 02:51:32.067660: step 3780/9000 (epoch 21/50), acc = 0.80, loss = 0.23 (12.9 examples/sec; 0.386 sec/batch), lr: 0.010000
2016-08-28 02:51:38.224676: step 3790/9000 (epoch 22/50), acc = 1.00, loss = 0.04 (10.6 examples/sec; 0.470 sec/batch), lr: 0.010000
2016-08-28 02:51:45.311604: step 3800/9000 (epoch 22/50), acc = 1.00, loss = 0.09 (11.1 examples/sec; 0.450 sec/batch), lr: 0.010000

Step 3800: train_loss = 0.094966, train_accuracy = 1.000
Step 3800: dev_loss = 0.423610, dev_accuracy = 0.820

2016-08-28 02:52:02.344983: step 3810/9000 (epoch 22/50), acc = 1.00, loss = 0.11 (12.9 examples/sec; 0.388 sec/batch), lr: 0.010000
2016-08-28 02:52:08.332351: step 3820/9000 (epoch 22/50), acc = 1.00, loss = 0.07 (13.3 examples/sec; 0.375 sec/batch), lr: 0.010000
2016-08-28 02:52:14.495513: step 3830/9000 (epoch 22/50), acc = 1.00, loss = 0.09 (10.4 examples/sec; 0.479 sec/batch), lr: 0.010000
2016-08-28 02:52:21.427848: step 3840/9000 (epoch 22/50), acc = 1.00, loss = 0.06 (13.7 examples/sec; 0.366 sec/batch), lr: 0.010000
2016-08-28 02:52:27.485542: step 3850/9000 (epoch 22/50), acc = 1.00, loss = 0.07 (13.6 examples/sec; 0.369 sec/batch), lr: 0.010000

Step 3850: train_loss = 0.082842, train_accuracy = 1.000
Step 3850: dev_loss = 0.423058, dev_accuracy = 0.820

2016-08-28 02:52:43.395187: step 3860/9000 (epoch 22/50), acc = 1.00, loss = 0.10 (14.3 examples/sec; 0.350 sec/batch), lr: 0.010000
2016-08-28 02:52:49.579616: step 3870/9000 (epoch 22/50), acc = 1.00, loss = 0.04 (13.1 examples/sec; 0.383 sec/batch), lr: 0.010000
2016-08-28 02:52:55.630658: step 3880/9000 (epoch 22/50), acc = 1.00, loss = 0.09 (14.1 examples/sec; 0.354 sec/batch), lr: 0.010000
2016-08-28 02:53:02.238318: step 3890/9000 (epoch 22/50), acc = 1.00, loss = 0.06 (9.1 examples/sec; 0.548 sec/batch), lr: 0.010000
2016-08-28 02:53:08.890713: step 3900/9000 (epoch 22/50), acc = 1.00, loss = 0.16 (14.3 examples/sec; 0.350 sec/batch), lr: 0.010000

Step 3900: train_loss = 0.083572, train_accuracy = 0.998
Step 3900: dev_loss = 0.395301, dev_accuracy = 0.830

2016-08-28 02:53:24.279511: step 3910/9000 (epoch 22/50), acc = 1.00, loss = 0.05 (14.2 examples/sec; 0.353 sec/batch), lr: 0.010000
2016-08-28 02:53:30.340107: step 3920/9000 (epoch 22/50), acc = 1.00, loss = 0.17 (13.4 examples/sec; 0.373 sec/batch), lr: 0.010000
2016-08-28 02:53:36.251603: step 3930/9000 (epoch 22/50), acc = 1.00, loss = 0.07 (13.7 examples/sec; 0.365 sec/batch), lr: 0.010000
2016-08-28 02:53:42.980707: step 3940/9000 (epoch 22/50), acc = 1.00, loss = 0.11 (8.7 examples/sec; 0.573 sec/batch), lr: 0.010000
2016-08-28 02:53:48.984113: step 3950/9000 (epoch 22/50), acc = 1.00, loss = 0.04 (14.0 examples/sec; 0.358 sec/batch), lr: 0.010000

Step 3950: train_loss = 0.082890, train_accuracy = 0.998
Step 3950: dev_loss = 0.393789, dev_accuracy = 0.840

2016-08-28 02:54:06.276428: step 3960/9000 (epoch 22/50), acc = 1.00, loss = 0.08 (11.9 examples/sec; 0.421 sec/batch), lr: 0.010000
2016-08-28 02:54:12.466504: step 3970/9000 (epoch 23/50), acc = 1.00, loss = 0.04 (13.8 examples/sec; 0.363 sec/batch), lr: 0.010000
2016-08-28 02:54:18.448713: step 3980/9000 (epoch 23/50), acc = 1.00, loss = 0.04 (13.4 examples/sec; 0.372 sec/batch), lr: 0.010000
2016-08-28 02:54:24.436834: step 3990/9000 (epoch 23/50), acc = 1.00, loss = 0.12 (12.6 examples/sec; 0.395 sec/batch), lr: 0.010000
2016-08-28 02:54:30.606483: step 4000/9000 (epoch 23/50), acc = 1.00, loss = 0.11 (13.2 examples/sec; 0.378 sec/batch), lr: 0.010000

Step 4000: train_loss = 0.078336, train_accuracy = 1.000
Step 4000: dev_loss = 0.428709, dev_accuracy = 0.810

2016-08-28 02:54:47.143759: step 4010/9000 (epoch 23/50), acc = 1.00, loss = 0.06 (13.0 examples/sec; 0.386 sec/batch), lr: 0.010000
2016-08-28 02:54:54.155635: step 4020/9000 (epoch 23/50), acc = 1.00, loss = 0.02 (12.8 examples/sec; 0.392 sec/batch), lr: 0.010000
2016-08-28 02:55:01.237912: step 4030/9000 (epoch 23/50), acc = 1.00, loss = 0.04 (13.0 examples/sec; 0.385 sec/batch), lr: 0.010000
2016-08-28 02:55:08.161742: step 4040/9000 (epoch 23/50), acc = 1.00, loss = 0.05 (13.1 examples/sec; 0.382 sec/batch), lr: 0.010000
2016-08-28 02:55:14.351047: step 4050/9000 (epoch 23/50), acc = 1.00, loss = 0.04 (11.8 examples/sec; 0.424 sec/batch), lr: 0.010000

Step 4050: train_loss = 0.074039, train_accuracy = 1.000
Step 4050: dev_loss = 0.391487, dev_accuracy = 0.800

2016-08-28 02:55:30.939345: step 4060/9000 (epoch 23/50), acc = 1.00, loss = 0.03 (12.0 examples/sec; 0.417 sec/batch), lr: 0.010000
2016-08-28 02:55:37.759664: step 4070/9000 (epoch 23/50), acc = 1.00, loss = 0.04 (13.7 examples/sec; 0.366 sec/batch), lr: 0.010000
2016-08-28 02:55:44.099198: step 4080/9000 (epoch 23/50), acc = 1.00, loss = 0.07 (9.1 examples/sec; 0.550 sec/batch), lr: 0.010000
2016-08-28 02:55:51.675441: step 4090/9000 (epoch 23/50), acc = 1.00, loss = 0.07 (11.3 examples/sec; 0.443 sec/batch), lr: 0.010000
2016-08-28 02:55:58.230697: step 4100/9000 (epoch 23/50), acc = 1.00, loss = 0.04 (12.1 examples/sec; 0.412 sec/batch), lr: 0.010000

Step 4100: train_loss = 0.076720, train_accuracy = 0.999
Step 4100: dev_loss = 0.396491, dev_accuracy = 0.820

2016-08-28 02:56:14.375249: step 4110/9000 (epoch 23/50), acc = 1.00, loss = 0.10 (14.1 examples/sec; 0.356 sec/batch), lr: 0.010000
2016-08-28 02:56:20.613100: step 4120/9000 (epoch 23/50), acc = 0.80, loss = 0.30 (13.4 examples/sec; 0.372 sec/batch), lr: 0.010000
2016-08-28 02:56:26.625926: step 4130/9000 (epoch 23/50), acc = 1.00, loss = 0.10 (13.5 examples/sec; 0.370 sec/batch), lr: 0.010000
2016-08-28 02:56:32.805421: step 4140/9000 (epoch 23/50), acc = 1.00, loss = 0.04 (11.1 examples/sec; 0.451 sec/batch), lr: 0.010000
2016-08-28 02:56:39.108392: step 4150/9000 (epoch 24/50), acc = 1.00, loss = 0.07 (13.8 examples/sec; 0.363 sec/batch), lr: 0.010000

Step 4150: train_loss = 0.063192, train_accuracy = 1.000
Step 4150: dev_loss = 0.393561, dev_accuracy = 0.830

2016-08-28 02:56:55.834649: step 4160/9000 (epoch 24/50), acc = 1.00, loss = 0.07 (14.4 examples/sec; 0.347 sec/batch), lr: 0.010000
2016-08-28 02:57:02.556559: step 4170/9000 (epoch 24/50), acc = 1.00, loss = 0.07 (12.4 examples/sec; 0.404 sec/batch), lr: 0.010000
2016-08-28 02:57:03.246495: step 4171/9000 (epoch 24/50), LR decays to 0.00950
2016-08-28 02:57:08.675438: step 4180/9000 (epoch 24/50), acc = 1.00, loss = 0.08 (14.4 examples/sec; 0.348 sec/batch), lr: 0.009500
2016-08-28 02:57:14.731648: step 4190/9000 (epoch 24/50), acc = 1.00, loss = 0.07 (13.9 examples/sec; 0.359 sec/batch), lr: 0.009500
2016-08-28 02:57:20.813274: step 4200/9000 (epoch 24/50), acc = 1.00, loss = 0.01 (14.2 examples/sec; 0.351 sec/batch), lr: 0.009500

Step 4200: train_loss = 0.061098, train_accuracy = 0.997
Step 4200: dev_loss = 0.395662, dev_accuracy = 0.840

2016-08-28 02:57:36.867564: step 4210/9000 (epoch 24/50), acc = 1.00, loss = 0.04 (14.3 examples/sec; 0.350 sec/batch), lr: 0.009500
2016-08-28 02:57:43.205109: step 4220/9000 (epoch 24/50), acc = 1.00, loss = 0.15 (12.7 examples/sec; 0.393 sec/batch), lr: 0.009500
2016-08-28 02:57:49.558621: step 4230/9000 (epoch 24/50), acc = 1.00, loss = 0.07 (11.9 examples/sec; 0.421 sec/batch), lr: 0.009500
2016-08-28 02:57:56.683270: step 4240/9000 (epoch 24/50), acc = 1.00, loss = 0.09 (10.4 examples/sec; 0.479 sec/batch), lr: 0.009500
2016-08-28 02:58:03.457976: step 4250/9000 (epoch 24/50), acc = 1.00, loss = 0.11 (12.7 examples/sec; 0.394 sec/batch), lr: 0.009500

Step 4250: train_loss = 0.059384, train_accuracy = 0.996
Step 4250: dev_loss = 0.388766, dev_accuracy = 0.810

2016-08-28 02:58:21.452593: step 4260/9000 (epoch 24/50), acc = 1.00, loss = 0.07 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009500
2016-08-28 02:58:27.852762: step 4270/9000 (epoch 24/50), acc = 1.00, loss = 0.09 (11.7 examples/sec; 0.427 sec/batch), lr: 0.009500
2016-08-28 02:58:34.341896: step 4280/9000 (epoch 24/50), acc = 1.00, loss = 0.03 (12.4 examples/sec; 0.402 sec/batch), lr: 0.009500
2016-08-28 02:58:40.639914: step 4290/9000 (epoch 24/50), acc = 1.00, loss = 0.05 (13.7 examples/sec; 0.366 sec/batch), lr: 0.009500
2016-08-28 02:58:47.098656: step 4300/9000 (epoch 24/50), acc = 1.00, loss = 0.08 (13.5 examples/sec; 0.370 sec/batch), lr: 0.009500

Step 4300: train_loss = 0.061411, train_accuracy = 0.998
Step 4300: dev_loss = 0.392077, dev_accuracy = 0.840

2016-08-28 02:59:03.867321: step 4310/9000 (epoch 24/50), acc = 1.00, loss = 0.09 (14.3 examples/sec; 0.349 sec/batch), lr: 0.009500
2016-08-28 02:59:10.803890: step 4320/9000 (epoch 24/50), acc = 1.00, loss = 0.08 (12.7 examples/sec; 0.394 sec/batch), lr: 0.009500
2016-08-28 02:59:17.480927: step 4330/9000 (epoch 25/50), acc = 1.00, loss = 0.06 (12.5 examples/sec; 0.401 sec/batch), lr: 0.009500
2016-08-28 02:59:24.072941: step 4340/9000 (epoch 25/50), acc = 1.00, loss = 0.07 (12.8 examples/sec; 0.391 sec/batch), lr: 0.009500
2016-08-28 02:59:32.249362: step 4350/9000 (epoch 25/50), acc = 1.00, loss = 0.06 (11.4 examples/sec; 0.439 sec/batch), lr: 0.009500

Step 4350: train_loss = 0.070672, train_accuracy = 1.000
Step 4350: dev_loss = 0.426544, dev_accuracy = 0.810

2016-08-28 02:59:49.798285: step 4360/9000 (epoch 25/50), acc = 1.00, loss = 0.09 (12.8 examples/sec; 0.390 sec/batch), lr: 0.009500
2016-08-28 02:59:56.349687: step 4370/9000 (epoch 25/50), acc = 1.00, loss = 0.15 (12.5 examples/sec; 0.401 sec/batch), lr: 0.009500
2016-08-28 03:00:03.584328: step 4380/9000 (epoch 25/50), acc = 1.00, loss = 0.02 (9.4 examples/sec; 0.533 sec/batch), lr: 0.009500
2016-08-28 03:00:10.406467: step 4390/9000 (epoch 25/50), acc = 1.00, loss = 0.06 (13.0 examples/sec; 0.383 sec/batch), lr: 0.009500
2016-08-28 03:00:17.390734: step 4400/9000 (epoch 25/50), acc = 1.00, loss = 0.04 (11.4 examples/sec; 0.437 sec/batch), lr: 0.009500

Step 4400: train_loss = 0.064809, train_accuracy = 1.000
Step 4400: dev_loss = 0.397138, dev_accuracy = 0.840

2016-08-28 03:00:35.533878: step 4410/9000 (epoch 25/50), acc = 1.00, loss = 0.05 (12.7 examples/sec; 0.394 sec/batch), lr: 0.009500
2016-08-28 03:00:42.029441: step 4420/9000 (epoch 25/50), acc = 1.00, loss = 0.08 (11.7 examples/sec; 0.427 sec/batch), lr: 0.009500
2016-08-28 03:00:48.986538: step 4430/9000 (epoch 25/50), acc = 1.00, loss = 0.20 (13.9 examples/sec; 0.360 sec/batch), lr: 0.009500
2016-08-28 03:00:55.114072: step 4440/9000 (epoch 25/50), acc = 1.00, loss = 0.14 (13.7 examples/sec; 0.364 sec/batch), lr: 0.009500
2016-08-28 03:01:01.353417: step 4450/9000 (epoch 25/50), acc = 1.00, loss = 0.11 (14.0 examples/sec; 0.358 sec/batch), lr: 0.009500

Step 4450: train_loss = 0.067606, train_accuracy = 1.000
Step 4450: dev_loss = 0.385386, dev_accuracy = 0.810

2016-08-28 03:01:18.091139: step 4460/9000 (epoch 25/50), acc = 1.00, loss = 0.08 (13.7 examples/sec; 0.365 sec/batch), lr: 0.009500
2016-08-28 03:01:25.020098: step 4470/9000 (epoch 25/50), acc = 1.00, loss = 0.07 (13.2 examples/sec; 0.379 sec/batch), lr: 0.009500
2016-08-28 03:01:31.789893: step 4480/9000 (epoch 25/50), acc = 1.00, loss = 0.06 (13.0 examples/sec; 0.385 sec/batch), lr: 0.009500
2016-08-28 03:01:37.989029: step 4490/9000 (epoch 25/50), acc = 1.00, loss = 0.07 (13.5 examples/sec; 0.371 sec/batch), lr: 0.009500
2016-08-28 03:01:44.395842: step 4500/9000 (epoch 25/50), acc = 1.00, loss = 0.04 (12.6 examples/sec; 0.397 sec/batch), lr: 0.009500

Step 4500: train_loss = 0.064924, train_accuracy = 1.000
Step 4500: dev_loss = 0.385366, dev_accuracy = 0.840

2016-08-28 03:02:02.471556: step 4510/9000 (epoch 26/50), acc = 1.00, loss = 0.08 (13.3 examples/sec; 0.377 sec/batch), lr: 0.009500
2016-08-28 03:02:08.904114: step 4520/9000 (epoch 26/50), acc = 1.00, loss = 0.06 (13.0 examples/sec; 0.385 sec/batch), lr: 0.009500
2016-08-28 03:02:15.257328: step 4530/9000 (epoch 26/50), acc = 1.00, loss = 0.03 (12.4 examples/sec; 0.405 sec/batch), lr: 0.009500
2016-08-28 03:02:21.601531: step 4540/9000 (epoch 26/50), acc = 1.00, loss = 0.15 (13.7 examples/sec; 0.365 sec/batch), lr: 0.009500
2016-08-28 03:02:27.746176: step 4550/9000 (epoch 26/50), acc = 1.00, loss = 0.05 (13.7 examples/sec; 0.364 sec/batch), lr: 0.009500

Step 4550: train_loss = 0.056424, train_accuracy = 1.000
Step 4550: dev_loss = 0.402166, dev_accuracy = 0.850

2016-08-28 03:02:45.296204: step 4560/9000 (epoch 26/50), acc = 1.00, loss = 0.04 (12.3 examples/sec; 0.405 sec/batch), lr: 0.009500
2016-08-28 03:02:52.153585: step 4570/9000 (epoch 26/50), acc = 1.00, loss = 0.05 (12.5 examples/sec; 0.400 sec/batch), lr: 0.009500
2016-08-28 03:02:58.454324: step 4580/9000 (epoch 26/50), acc = 1.00, loss = 0.03 (13.8 examples/sec; 0.363 sec/batch), lr: 0.009500
2016-08-28 03:03:05.121512: step 4590/9000 (epoch 26/50), acc = 1.00, loss = 0.05 (11.0 examples/sec; 0.454 sec/batch), lr: 0.009500
2016-08-28 03:03:11.615119: step 4600/9000 (epoch 26/50), acc = 1.00, loss = 0.11 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009500

Step 4600: train_loss = 0.055323, train_accuracy = 1.000
Step 4600: dev_loss = 0.386735, dev_accuracy = 0.830

2016-08-28 03:03:30.309180: step 4610/9000 (epoch 26/50), acc = 1.00, loss = 0.02 (12.5 examples/sec; 0.401 sec/batch), lr: 0.009500
2016-08-28 03:03:36.802517: step 4620/9000 (epoch 26/50), acc = 1.00, loss = 0.08 (14.0 examples/sec; 0.356 sec/batch), lr: 0.009500
2016-08-28 03:03:43.086086: step 4630/9000 (epoch 26/50), acc = 1.00, loss = 0.06 (13.6 examples/sec; 0.368 sec/batch), lr: 0.009500
2016-08-28 03:03:49.746751: step 4640/9000 (epoch 26/50), acc = 1.00, loss = 0.03 (12.6 examples/sec; 0.398 sec/batch), lr: 0.009500
2016-08-28 03:03:56.552133: step 4650/9000 (epoch 26/50), acc = 1.00, loss = 0.03 (12.3 examples/sec; 0.405 sec/batch), lr: 0.009500

Step 4650: train_loss = 0.055095, train_accuracy = 1.000
Step 4650: dev_loss = 0.384581, dev_accuracy = 0.820

2016-08-28 03:04:14.654915: step 4660/9000 (epoch 26/50), acc = 1.00, loss = 0.04 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009500
2016-08-28 03:04:22.100928: step 4670/9000 (epoch 26/50), acc = 1.00, loss = 0.04 (12.4 examples/sec; 0.402 sec/batch), lr: 0.009500
2016-08-28 03:04:28.975497: step 4680/9000 (epoch 26/50), acc = 1.00, loss = 0.03 (12.3 examples/sec; 0.405 sec/batch), lr: 0.009500
2016-08-28 03:04:35.835869: step 4690/9000 (epoch 27/50), acc = 1.00, loss = 0.05 (13.7 examples/sec; 0.366 sec/batch), lr: 0.009500
2016-08-28 03:04:42.231937: step 4700/9000 (epoch 27/50), acc = 1.00, loss = 0.06 (13.7 examples/sec; 0.364 sec/batch), lr: 0.009500

Step 4700: train_loss = 0.061537, train_accuracy = 1.000
Step 4700: dev_loss = 0.395584, dev_accuracy = 0.840

2016-08-28 03:04:59.920755: step 4710/9000 (epoch 27/50), acc = 1.00, loss = 0.02 (14.2 examples/sec; 0.353 sec/batch), lr: 0.009500
2016-08-28 03:05:06.561718: step 4720/9000 (epoch 27/50), acc = 1.00, loss = 0.04 (12.8 examples/sec; 0.390 sec/batch), lr: 0.009500
2016-08-28 03:05:13.010133: step 4730/9000 (epoch 27/50), acc = 1.00, loss = 0.07 (14.2 examples/sec; 0.353 sec/batch), lr: 0.009500
2016-08-28 03:05:19.396103: step 4740/9000 (epoch 27/50), acc = 1.00, loss = 0.06 (11.4 examples/sec; 0.438 sec/batch), lr: 0.009500
2016-08-28 03:05:26.354462: step 4750/9000 (epoch 27/50), acc = 1.00, loss = 0.05 (13.5 examples/sec; 0.371 sec/batch), lr: 0.009500

Step 4750: train_loss = 0.054813, train_accuracy = 1.000
Step 4750: dev_loss = 0.405525, dev_accuracy = 0.840

2016-08-28 03:05:45.714254: step 4760/9000 (epoch 27/50), acc = 1.00, loss = 0.02 (9.2 examples/sec; 0.544 sec/batch), lr: 0.009500
2016-08-28 03:05:52.830569: step 4770/9000 (epoch 27/50), acc = 1.00, loss = 0.06 (14.1 examples/sec; 0.353 sec/batch), lr: 0.009500
2016-08-28 03:05:59.157277: step 4780/9000 (epoch 27/50), acc = 1.00, loss = 0.02 (11.5 examples/sec; 0.433 sec/batch), lr: 0.009500
2016-08-28 03:06:06.009330: step 4790/9000 (epoch 27/50), acc = 1.00, loss = 0.02 (13.4 examples/sec; 0.373 sec/batch), lr: 0.009500
2016-08-28 03:06:13.293950: step 4800/9000 (epoch 27/50), acc = 1.00, loss = 0.07 (13.5 examples/sec; 0.371 sec/batch), lr: 0.009500

Step 4800: train_loss = 0.054936, train_accuracy = 1.000
Step 4800: dev_loss = 0.381278, dev_accuracy = 0.810

2016-08-28 03:06:31.875674: step 4810/9000 (epoch 27/50), acc = 1.00, loss = 0.02 (12.9 examples/sec; 0.386 sec/batch), lr: 0.009500
2016-08-28 03:06:38.293929: step 4820/9000 (epoch 27/50), acc = 1.00, loss = 0.04 (11.8 examples/sec; 0.423 sec/batch), lr: 0.009500
2016-08-28 03:06:45.331310: step 4830/9000 (epoch 27/50), acc = 1.00, loss = 0.07 (13.2 examples/sec; 0.378 sec/batch), lr: 0.009500
2016-08-28 03:06:52.663139: step 4840/9000 (epoch 27/50), acc = 1.00, loss = 0.04 (12.0 examples/sec; 0.417 sec/batch), lr: 0.009500
2016-08-28 03:06:59.626717: step 4850/9000 (epoch 27/50), acc = 1.00, loss = 0.02 (13.4 examples/sec; 0.373 sec/batch), lr: 0.009500

Step 4850: train_loss = 0.055611, train_accuracy = 1.000
Step 4850: dev_loss = 0.381225, dev_accuracy = 0.810

2016-08-28 03:07:18.704163: step 4860/9000 (epoch 27/50), acc = 1.00, loss = 0.06 (12.8 examples/sec; 0.391 sec/batch), lr: 0.009500
2016-08-28 03:07:25.397517: step 4870/9000 (epoch 28/50), acc = 1.00, loss = 0.02 (13.5 examples/sec; 0.372 sec/batch), lr: 0.009500
2016-08-28 03:07:31.732124: step 4880/9000 (epoch 28/50), acc = 1.00, loss = 0.03 (13.3 examples/sec; 0.375 sec/batch), lr: 0.009500
2016-08-28 03:07:38.713856: step 4890/9000 (epoch 28/50), acc = 1.00, loss = 0.03 (10.8 examples/sec; 0.462 sec/batch), lr: 0.009500
2016-08-28 03:07:45.602952: step 4900/9000 (epoch 28/50), acc = 1.00, loss = 0.09 (13.4 examples/sec; 0.375 sec/batch), lr: 0.009500

Step 4900: train_loss = 0.054158, train_accuracy = 1.000
Step 4900: dev_loss = 0.402248, dev_accuracy = 0.840

2016-08-28 03:08:03.229111: step 4910/9000 (epoch 28/50), acc = 1.00, loss = 0.04 (11.5 examples/sec; 0.435 sec/batch), lr: 0.009500
2016-08-28 03:08:09.832555: step 4920/9000 (epoch 28/50), acc = 1.00, loss = 0.01 (13.8 examples/sec; 0.362 sec/batch), lr: 0.009500
2016-08-28 03:08:16.667719: step 4930/9000 (epoch 28/50), acc = 1.00, loss = 0.03 (13.0 examples/sec; 0.383 sec/batch), lr: 0.009500
2016-08-28 03:08:23.223991: step 4940/9000 (epoch 28/50), acc = 1.00, loss = 0.09 (13.5 examples/sec; 0.372 sec/batch), lr: 0.009500
2016-08-28 03:08:29.713813: step 4950/9000 (epoch 28/50), acc = 1.00, loss = 0.07 (13.2 examples/sec; 0.378 sec/batch), lr: 0.009500

Step 4950: train_loss = 0.047430, train_accuracy = 1.000
Step 4950: dev_loss = 0.379974, dev_accuracy = 0.810

2016-08-28 03:08:47.973470: step 4960/9000 (epoch 28/50), acc = 1.00, loss = 0.03 (13.2 examples/sec; 0.378 sec/batch), lr: 0.009500
2016-08-28 03:08:54.630921: step 4970/9000 (epoch 28/50), acc = 1.00, loss = 0.03 (13.7 examples/sec; 0.366 sec/batch), lr: 0.009500
2016-08-28 03:09:00.922922: step 4980/9000 (epoch 28/50), acc = 1.00, loss = 0.06 (14.1 examples/sec; 0.355 sec/batch), lr: 0.009500
2016-08-28 03:09:07.257345: step 4990/9000 (epoch 28/50), acc = 1.00, loss = 0.05 (13.5 examples/sec; 0.372 sec/batch), lr: 0.009500
2016-08-28 03:09:14.042166: step 5000/9000 (epoch 28/50), acc = 1.00, loss = 0.02 (14.1 examples/sec; 0.354 sec/batch), lr: 0.009500

Step 5000: train_loss = 0.048671, train_accuracy = 1.000
Step 5000: dev_loss = 0.378396, dev_accuracy = 0.810

2016-08-28 03:09:31.757603: step 5010/9000 (epoch 28/50), acc = 1.00, loss = 0.07 (12.5 examples/sec; 0.400 sec/batch), lr: 0.009500
2016-08-28 03:09:38.580663: step 5020/9000 (epoch 28/50), acc = 1.00, loss = 0.03 (13.1 examples/sec; 0.383 sec/batch), lr: 0.009500
2016-08-28 03:09:46.525583: step 5030/9000 (epoch 28/50), acc = 1.00, loss = 0.02 (10.3 examples/sec; 0.484 sec/batch), lr: 0.009500
2016-08-28 03:09:54.356628: step 5040/9000 (epoch 28/50), acc = 1.00, loss = 0.06 (9.5 examples/sec; 0.524 sec/batch), lr: 0.009500
2016-08-28 03:10:03.332766: step 5050/9000 (epoch 29/50), acc = 1.00, loss = 0.04 (9.2 examples/sec; 0.542 sec/batch), lr: 0.009500

Step 5050: train_loss = 0.049066, train_accuracy = 1.000
Step 5050: dev_loss = 0.381354, dev_accuracy = 0.820

2016-08-28 03:10:21.832083: step 5060/9000 (epoch 29/50), acc = 1.00, loss = 0.03 (14.2 examples/sec; 0.352 sec/batch), lr: 0.009500
2016-08-28 03:10:28.213766: step 5070/9000 (epoch 29/50), acc = 1.00, loss = 0.06 (13.9 examples/sec; 0.360 sec/batch), lr: 0.009500
2016-08-28 03:10:34.952241: step 5080/9000 (epoch 29/50), acc = 1.00, loss = 0.06 (13.2 examples/sec; 0.380 sec/batch), lr: 0.009500
2016-08-28 03:10:41.563805: step 5090/9000 (epoch 29/50), acc = 1.00, loss = 0.01 (14.1 examples/sec; 0.355 sec/batch), lr: 0.009500
2016-08-28 03:10:47.879284: step 5100/9000 (epoch 29/50), acc = 1.00, loss = 0.01 (14.1 examples/sec; 0.354 sec/batch), lr: 0.009500

Step 5100: train_loss = 0.054182, train_accuracy = 1.000
Step 5100: dev_loss = 0.398268, dev_accuracy = 0.840

2016-08-28 03:11:07.521484: step 5110/9000 (epoch 29/50), acc = 1.00, loss = 0.06 (11.7 examples/sec; 0.429 sec/batch), lr: 0.009500
2016-08-28 03:11:14.427235: step 5120/9000 (epoch 29/50), acc = 1.00, loss = 0.03 (13.2 examples/sec; 0.379 sec/batch), lr: 0.009500
2016-08-28 03:11:22.021777: step 5130/9000 (epoch 29/50), acc = 1.00, loss = 0.06 (12.7 examples/sec; 0.393 sec/batch), lr: 0.009500
2016-08-28 03:11:29.758671: step 5140/9000 (epoch 29/50), acc = 1.00, loss = 0.02 (13.0 examples/sec; 0.386 sec/batch), lr: 0.009500
2016-08-28 03:11:36.598484: step 5150/9000 (epoch 29/50), acc = 1.00, loss = 0.01 (13.1 examples/sec; 0.383 sec/batch), lr: 0.009500

Step 5150: train_loss = 0.051661, train_accuracy = 1.000
Step 5150: dev_loss = 0.385469, dev_accuracy = 0.840

2016-08-28 03:11:56.940831: step 5160/9000 (epoch 29/50), acc = 1.00, loss = 0.07 (11.2 examples/sec; 0.448 sec/batch), lr: 0.009500
2016-08-28 03:12:04.139486: step 5170/9000 (epoch 29/50), acc = 1.00, loss = 0.03 (13.2 examples/sec; 0.378 sec/batch), lr: 0.009500
2016-08-28 03:12:11.234138: step 5180/9000 (epoch 29/50), acc = 1.00, loss = 0.02 (9.5 examples/sec; 0.526 sec/batch), lr: 0.009500
2016-08-28 03:12:18.188827: step 5190/9000 (epoch 29/50), acc = 1.00, loss = 0.09 (13.7 examples/sec; 0.366 sec/batch), lr: 0.009500
2016-08-28 03:12:24.673758: step 5200/9000 (epoch 29/50), acc = 1.00, loss = 0.06 (13.6 examples/sec; 0.368 sec/batch), lr: 0.009500

Step 5200: train_loss = 0.050663, train_accuracy = 1.000
Step 5200: dev_loss = 0.377840, dev_accuracy = 0.810

2016-08-28 03:12:44.185952: step 5210/9000 (epoch 29/50), acc = 1.00, loss = 0.02 (12.6 examples/sec; 0.396 sec/batch), lr: 0.009500
2016-08-28 03:12:51.767562: step 5220/9000 (epoch 29/50), acc = 1.00, loss = 0.02 (13.2 examples/sec; 0.380 sec/batch), lr: 0.009500
2016-08-28 03:12:58.825661: step 5230/9000 (epoch 30/50), acc = 1.00, loss = 0.02 (13.5 examples/sec; 0.369 sec/batch), lr: 0.009500
2016-08-28 03:13:06.017172: step 5240/9000 (epoch 30/50), acc = 1.00, loss = 0.02 (13.1 examples/sec; 0.381 sec/batch), lr: 0.009500
2016-08-28 03:13:13.579072: step 5250/9000 (epoch 30/50), acc = 1.00, loss = 0.05 (13.1 examples/sec; 0.383 sec/batch), lr: 0.009500

Step 5250: train_loss = 0.054242, train_accuracy = 1.000
Step 5250: dev_loss = 0.416117, dev_accuracy = 0.810

2016-08-28 03:13:34.892851: step 5260/9000 (epoch 30/50), acc = 1.00, loss = 0.04 (12.6 examples/sec; 0.396 sec/batch), lr: 0.009500
2016-08-28 03:13:41.963695: step 5270/9000 (epoch 30/50), acc = 1.00, loss = 0.01 (12.8 examples/sec; 0.390 sec/batch), lr: 0.009500
2016-08-28 03:13:49.076688: step 5280/9000 (epoch 30/50), acc = 1.00, loss = 0.01 (12.7 examples/sec; 0.394 sec/batch), lr: 0.009500
2016-08-28 03:13:56.423607: step 5290/9000 (epoch 30/50), acc = 1.00, loss = 0.02 (11.3 examples/sec; 0.442 sec/batch), lr: 0.009500
2016-08-28 03:14:03.710443: step 5300/9000 (epoch 30/50), acc = 1.00, loss = 0.02 (13.6 examples/sec; 0.368 sec/batch), lr: 0.009500

Step 5300: train_loss = 0.044515, train_accuracy = 1.000
Step 5300: dev_loss = 0.387057, dev_accuracy = 0.840

2016-08-28 03:14:24.371866: step 5310/9000 (epoch 30/50), acc = 1.00, loss = 0.02 (10.0 examples/sec; 0.499 sec/batch), lr: 0.009500
2016-08-28 03:14:32.022138: step 5320/9000 (epoch 30/50), acc = 1.00, loss = 0.01 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009500
2016-08-28 03:14:38.745263: step 5330/9000 (epoch 30/50), acc = 1.00, loss = 0.02 (12.6 examples/sec; 0.397 sec/batch), lr: 0.009500
2016-08-28 03:14:46.437802: step 5340/9000 (epoch 30/50), acc = 1.00, loss = 0.04 (12.4 examples/sec; 0.404 sec/batch), lr: 0.009500
2016-08-28 03:14:53.830401: step 5350/9000 (epoch 30/50), acc = 1.00, loss = 0.04 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009500

Step 5350: train_loss = 0.044756, train_accuracy = 1.000
Step 5350: dev_loss = 0.378982, dev_accuracy = 0.820

2016-08-28 03:15:13.411864: step 5360/9000 (epoch 30/50), acc = 1.00, loss = 0.04 (13.2 examples/sec; 0.378 sec/batch), lr: 0.009500
2016-08-28 03:15:20.858991: step 5370/9000 (epoch 30/50), acc = 1.00, loss = 0.06 (13.6 examples/sec; 0.368 sec/batch), lr: 0.009500
2016-08-28 03:15:27.562927: step 5380/9000 (epoch 30/50), acc = 1.00, loss = 0.03 (12.5 examples/sec; 0.400 sec/batch), lr: 0.009500
2016-08-28 03:15:34.753468: step 5390/9000 (epoch 30/50), acc = 1.00, loss = 0.06 (11.8 examples/sec; 0.423 sec/batch), lr: 0.009500
2016-08-28 03:15:42.053532: step 5400/9000 (epoch 30/50), acc = 1.00, loss = 0.04 (10.6 examples/sec; 0.472 sec/batch), lr: 0.009500

Step 5400: train_loss = 0.044543, train_accuracy = 1.000
Step 5400: dev_loss = 0.376832, dev_accuracy = 0.810

2016-08-28 03:16:01.793028: step 5410/9000 (epoch 31/50), acc = 1.00, loss = 0.05 (13.2 examples/sec; 0.379 sec/batch), lr: 0.009500
2016-08-28 03:16:09.038417: step 5420/9000 (epoch 31/50), acc = 1.00, loss = 0.06 (12.5 examples/sec; 0.399 sec/batch), lr: 0.009500
2016-08-28 03:16:09.038467: step 5420/9000 (epoch 31/50), LR decays to 0.00903
2016-08-28 03:16:16.697001: step 5430/9000 (epoch 31/50), acc = 1.00, loss = 0.04 (14.2 examples/sec; 0.353 sec/batch), lr: 0.009025
2016-08-28 03:16:23.516932: step 5440/9000 (epoch 31/50), acc = 1.00, loss = 0.03 (13.0 examples/sec; 0.386 sec/batch), lr: 0.009025
2016-08-28 03:16:30.249028: step 5450/9000 (epoch 31/50), acc = 1.00, loss = 0.01 (13.0 examples/sec; 0.383 sec/batch), lr: 0.009025

Step 5450: train_loss = 0.047108, train_accuracy = 0.996
Step 5450: dev_loss = 0.410641, dev_accuracy = 0.820

2016-08-28 03:16:48.903964: step 5460/9000 (epoch 31/50), acc = 1.00, loss = 0.04 (14.3 examples/sec; 0.349 sec/batch), lr: 0.009025
2016-08-28 03:16:56.546873: step 5470/9000 (epoch 31/50), acc = 1.00, loss = 0.03 (12.2 examples/sec; 0.410 sec/batch), lr: 0.009025
2016-08-28 03:17:03.634118: step 5480/9000 (epoch 31/50), acc = 1.00, loss = 0.01 (13.0 examples/sec; 0.384 sec/batch), lr: 0.009025
2016-08-28 03:17:10.321004: step 5490/9000 (epoch 31/50), acc = 1.00, loss = 0.04 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009025
2016-08-28 03:17:16.838244: step 5500/9000 (epoch 31/50), acc = 1.00, loss = 0.04 (13.4 examples/sec; 0.372 sec/batch), lr: 0.009025

Step 5500: train_loss = 0.042138, train_accuracy = 0.998
Step 5500: dev_loss = 0.380601, dev_accuracy = 0.840

2016-08-28 03:17:36.017619: step 5510/9000 (epoch 31/50), acc = 1.00, loss = 0.02 (13.2 examples/sec; 0.378 sec/batch), lr: 0.009025
2016-08-28 03:17:44.009353: step 5520/9000 (epoch 31/50), acc = 1.00, loss = 0.03 (12.8 examples/sec; 0.390 sec/batch), lr: 0.009025
2016-08-28 03:17:51.732477: step 5530/9000 (epoch 31/50), acc = 1.00, loss = 0.07 (12.3 examples/sec; 0.406 sec/batch), lr: 0.009025
2016-08-28 03:17:58.673000: step 5540/9000 (epoch 31/50), acc = 1.00, loss = 0.02 (13.6 examples/sec; 0.369 sec/batch), lr: 0.009025
2016-08-28 03:18:06.702955: step 5550/9000 (epoch 31/50), acc = 1.00, loss = 0.04 (13.0 examples/sec; 0.386 sec/batch), lr: 0.009025

Step 5550: train_loss = 0.041378, train_accuracy = 0.999
Step 5550: dev_loss = 0.376176, dev_accuracy = 0.830

2016-08-28 03:18:26.616076: step 5560/9000 (epoch 31/50), acc = 1.00, loss = 0.06 (13.9 examples/sec; 0.360 sec/batch), lr: 0.009025
2016-08-28 03:18:33.508742: step 5570/9000 (epoch 31/50), acc = 1.00, loss = 0.02 (12.8 examples/sec; 0.389 sec/batch), lr: 0.009025
2016-08-28 03:18:40.285892: step 5580/9000 (epoch 31/50), acc = 1.00, loss = 0.03 (13.5 examples/sec; 0.371 sec/batch), lr: 0.009025
2016-08-28 03:18:47.118583: step 5590/9000 (epoch 32/50), acc = 1.00, loss = 0.01 (12.4 examples/sec; 0.403 sec/batch), lr: 0.009025
2016-08-28 03:18:53.937447: step 5600/9000 (epoch 32/50), acc = 1.00, loss = 0.02 (13.8 examples/sec; 0.362 sec/batch), lr: 0.009025

Step 5600: train_loss = 0.039133, train_accuracy = 1.000
Step 5600: dev_loss = 0.385430, dev_accuracy = 0.840

2016-08-28 03:19:13.713780: step 5610/9000 (epoch 32/50), acc = 1.00, loss = 0.02 (11.4 examples/sec; 0.437 sec/batch), lr: 0.009025
2016-08-28 03:19:22.587556: step 5620/9000 (epoch 32/50), acc = 1.00, loss = 0.03 (12.3 examples/sec; 0.406 sec/batch), lr: 0.009025
2016-08-28 03:19:29.721224: step 5630/9000 (epoch 32/50), acc = 1.00, loss = 0.05 (12.6 examples/sec; 0.395 sec/batch), lr: 0.009025
2016-08-28 03:19:36.536174: step 5640/9000 (epoch 32/50), acc = 1.00, loss = 0.01 (13.2 examples/sec; 0.378 sec/batch), lr: 0.009025
2016-08-28 03:19:43.532580: step 5650/9000 (epoch 32/50), acc = 1.00, loss = 0.05 (14.2 examples/sec; 0.353 sec/batch), lr: 0.009025

Step 5650: train_loss = 0.042281, train_accuracy = 1.000
Step 5650: dev_loss = 0.402013, dev_accuracy = 0.820

2016-08-28 03:20:03.185977: step 5660/9000 (epoch 32/50), acc = 1.00, loss = 0.05 (13.0 examples/sec; 0.384 sec/batch), lr: 0.009025
2016-08-28 03:20:09.817618: step 5670/9000 (epoch 32/50), acc = 1.00, loss = 0.03 (13.7 examples/sec; 0.366 sec/batch), lr: 0.009025
2016-08-28 03:20:16.565567: step 5680/9000 (epoch 32/50), acc = 1.00, loss = 0.03 (13.9 examples/sec; 0.359 sec/batch), lr: 0.009025
2016-08-28 03:20:23.229185: step 5690/9000 (epoch 32/50), acc = 1.00, loss = 0.03 (13.8 examples/sec; 0.363 sec/batch), lr: 0.009025
2016-08-28 03:20:29.868419: step 5700/9000 (epoch 32/50), acc = 1.00, loss = 0.05 (13.8 examples/sec; 0.361 sec/batch), lr: 0.009025

Step 5700: train_loss = 0.039617, train_accuracy = 1.000
Step 5700: dev_loss = 0.378211, dev_accuracy = 0.840

2016-08-28 03:20:48.873414: step 5710/9000 (epoch 32/50), acc = 1.00, loss = 0.08 (14.7 examples/sec; 0.341 sec/batch), lr: 0.009025
2016-08-28 03:20:56.487217: step 5720/9000 (epoch 32/50), acc = 1.00, loss = 0.06 (10.0 examples/sec; 0.502 sec/batch), lr: 0.009025
2016-08-28 03:21:03.394025: step 5730/9000 (epoch 32/50), acc = 1.00, loss = 0.03 (14.5 examples/sec; 0.345 sec/batch), lr: 0.009025
2016-08-28 03:21:10.219869: step 5740/9000 (epoch 32/50), acc = 1.00, loss = 0.05 (12.3 examples/sec; 0.407 sec/batch), lr: 0.009025
2016-08-28 03:21:17.150976: step 5750/9000 (epoch 32/50), acc = 1.00, loss = 0.05 (14.2 examples/sec; 0.351 sec/batch), lr: 0.009025

Step 5750: train_loss = 0.041619, train_accuracy = 1.000
Step 5750: dev_loss = 0.372804, dev_accuracy = 0.820

2016-08-28 03:21:37.582347: step 5760/9000 (epoch 32/50), acc = 1.00, loss = 0.10 (11.0 examples/sec; 0.454 sec/batch), lr: 0.009025
2016-08-28 03:21:45.860630: step 5770/9000 (epoch 33/50), acc = 1.00, loss = 0.02 (13.1 examples/sec; 0.383 sec/batch), lr: 0.009025
2016-08-28 03:21:53.679734: step 5780/9000 (epoch 33/50), acc = 1.00, loss = 0.06 (12.2 examples/sec; 0.408 sec/batch), lr: 0.009025
2016-08-28 03:22:00.527697: step 5790/9000 (epoch 33/50), acc = 1.00, loss = 0.02 (11.4 examples/sec; 0.440 sec/batch), lr: 0.009025
2016-08-28 03:22:07.910603: step 5800/9000 (epoch 33/50), acc = 1.00, loss = 0.04 (12.6 examples/sec; 0.398 sec/batch), lr: 0.009025

Step 5800: train_loss = 0.042793, train_accuracy = 1.000
Step 5800: dev_loss = 0.396844, dev_accuracy = 0.840

2016-08-28 03:22:30.133625: step 5810/9000 (epoch 33/50), acc = 1.00, loss = 0.03 (11.1 examples/sec; 0.450 sec/batch), lr: 0.009025
2016-08-28 03:22:37.217719: step 5820/9000 (epoch 33/50), acc = 1.00, loss = 0.02 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009025
2016-08-28 03:22:44.334488: step 5830/9000 (epoch 33/50), acc = 1.00, loss = 0.04 (12.0 examples/sec; 0.417 sec/batch), lr: 0.009025
2016-08-28 03:22:51.574646: step 5840/9000 (epoch 33/50), acc = 1.00, loss = 0.01 (11.7 examples/sec; 0.426 sec/batch), lr: 0.009025
2016-08-28 03:22:59.471735: step 5850/9000 (epoch 33/50), acc = 1.00, loss = 0.04 (13.7 examples/sec; 0.364 sec/batch), lr: 0.009025

Step 5850: train_loss = 0.040200, train_accuracy = 1.000
Step 5850: dev_loss = 0.376116, dev_accuracy = 0.830

2016-08-28 03:23:20.847548: step 5860/9000 (epoch 33/50), acc = 1.00, loss = 0.05 (13.5 examples/sec; 0.370 sec/batch), lr: 0.009025
2016-08-28 03:23:28.546580: step 5870/9000 (epoch 33/50), acc = 1.00, loss = 0.03 (13.8 examples/sec; 0.363 sec/batch), lr: 0.009025
2016-08-28 03:23:35.721551: step 5880/9000 (epoch 33/50), acc = 1.00, loss = 0.07 (14.0 examples/sec; 0.357 sec/batch), lr: 0.009025
2016-08-28 03:23:42.611588: step 5890/9000 (epoch 33/50), acc = 1.00, loss = 0.02 (12.9 examples/sec; 0.387 sec/batch), lr: 0.009025
2016-08-28 03:23:49.596489: step 5900/9000 (epoch 33/50), acc = 1.00, loss = 0.02 (13.5 examples/sec; 0.370 sec/batch), lr: 0.009025

Step 5900: train_loss = 0.040242, train_accuracy = 1.000
Step 5900: dev_loss = 0.371811, dev_accuracy = 0.810

2016-08-28 03:24:10.932611: step 5910/9000 (epoch 33/50), acc = 1.00, loss = 0.04 (12.8 examples/sec; 0.391 sec/batch), lr: 0.009025
2016-08-28 03:24:18.443488: step 5920/9000 (epoch 33/50), acc = 1.00, loss = 0.09 (12.6 examples/sec; 0.397 sec/batch), lr: 0.009025
2016-08-28 03:24:25.981091: step 5930/9000 (epoch 33/50), acc = 1.00, loss = 0.06 (11.6 examples/sec; 0.430 sec/batch), lr: 0.009025
2016-08-28 03:24:33.993806: step 5940/9000 (epoch 33/50), acc = 1.00, loss = 0.02 (10.9 examples/sec; 0.457 sec/batch), lr: 0.009025
2016-08-28 03:24:41.145175: step 5950/9000 (epoch 34/50), acc = 1.00, loss = 0.02 (14.6 examples/sec; 0.343 sec/batch), lr: 0.009025

Step 5950: train_loss = 0.043635, train_accuracy = 1.000
Step 5950: dev_loss = 0.373115, dev_accuracy = 0.830

2016-08-28 03:25:01.625763: step 5960/9000 (epoch 34/50), acc = 1.00, loss = 0.02 (13.7 examples/sec; 0.366 sec/batch), lr: 0.009025
2016-08-28 03:25:08.749328: step 5970/9000 (epoch 34/50), acc = 1.00, loss = 0.04 (13.8 examples/sec; 0.363 sec/batch), lr: 0.009025
2016-08-28 03:25:15.896762: step 5980/9000 (epoch 34/50), acc = 1.00, loss = 0.01 (14.2 examples/sec; 0.351 sec/batch), lr: 0.009025
2016-08-28 03:25:22.911517: step 5990/9000 (epoch 34/50), acc = 1.00, loss = 0.00 (13.4 examples/sec; 0.374 sec/batch), lr: 0.009025
2016-08-28 03:25:29.873742: step 6000/9000 (epoch 34/50), acc = 1.00, loss = 0.01 (12.9 examples/sec; 0.387 sec/batch), lr: 0.009025

Step 6000: train_loss = 0.037551, train_accuracy = 0.997
Step 6000: dev_loss = 0.400078, dev_accuracy = 0.840

2016-08-28 03:25:52.203587: step 6010/9000 (epoch 34/50), acc = 1.00, loss = 0.06 (13.4 examples/sec; 0.372 sec/batch), lr: 0.009025
2016-08-28 03:26:00.642632: step 6020/9000 (epoch 34/50), acc = 1.00, loss = 0.02 (6.9 examples/sec; 0.724 sec/batch), lr: 0.009025
2016-08-28 03:26:08.850018: step 6030/9000 (epoch 34/50), acc = 1.00, loss = 0.02 (11.8 examples/sec; 0.422 sec/batch), lr: 0.009025
2016-08-28 03:26:17.516593: step 6040/9000 (epoch 34/50), acc = 1.00, loss = 0.03 (11.7 examples/sec; 0.426 sec/batch), lr: 0.009025
2016-08-28 03:26:26.422277: step 6050/9000 (epoch 34/50), acc = 1.00, loss = 0.02 (12.6 examples/sec; 0.397 sec/batch), lr: 0.009025

Step 6050: train_loss = 0.037388, train_accuracy = 0.998
Step 6050: dev_loss = 0.372750, dev_accuracy = 0.810

2016-08-28 03:26:51.614759: step 6060/9000 (epoch 34/50), acc = 1.00, loss = 0.02 (9.5 examples/sec; 0.524 sec/batch), lr: 0.009025
2016-08-28 03:26:59.443287: step 6070/9000 (epoch 34/50), acc = 1.00, loss = 0.02 (11.0 examples/sec; 0.455 sec/batch), lr: 0.009025
2016-08-28 03:27:07.251453: step 6080/9000 (epoch 34/50), acc = 1.00, loss = 0.03 (12.0 examples/sec; 0.415 sec/batch), lr: 0.009025
2016-08-28 03:27:15.576574: step 6090/9000 (epoch 34/50), acc = 1.00, loss = 0.06 (9.4 examples/sec; 0.534 sec/batch), lr: 0.009025
2016-08-28 03:27:23.222110: step 6100/9000 (epoch 34/50), acc = 1.00, loss = 0.03 (12.2 examples/sec; 0.409 sec/batch), lr: 0.009025

Step 6100: train_loss = 0.038561, train_accuracy = 0.999
Step 6100: dev_loss = 0.372554, dev_accuracy = 0.830

2016-08-28 03:27:43.625083: step 6110/9000 (epoch 34/50), acc = 1.00, loss = 0.03 (11.6 examples/sec; 0.433 sec/batch), lr: 0.009025
2016-08-28 03:27:51.243741: step 6120/9000 (epoch 34/50), acc = 1.00, loss = 0.02 (13.2 examples/sec; 0.378 sec/batch), lr: 0.009025
2016-08-28 03:27:58.194255: step 6130/9000 (epoch 35/50), acc = 1.00, loss = 0.04 (13.4 examples/sec; 0.373 sec/batch), lr: 0.009025
2016-08-28 03:28:07.338939: step 6140/9000 (epoch 35/50), acc = 1.00, loss = 0.01 (13.1 examples/sec; 0.381 sec/batch), lr: 0.009025
2016-08-28 03:28:14.255553: step 6150/9000 (epoch 35/50), acc = 1.00, loss = 0.01 (14.2 examples/sec; 0.352 sec/batch), lr: 0.009025

Step 6150: train_loss = 0.038832, train_accuracy = 1.000
Step 6150: dev_loss = 0.392915, dev_accuracy = 0.850

2016-08-28 03:28:36.171153: step 6160/9000 (epoch 35/50), acc = 1.00, loss = 0.02 (9.8 examples/sec; 0.511 sec/batch), lr: 0.009025
2016-08-28 03:28:44.529101: step 6170/9000 (epoch 35/50), acc = 1.00, loss = 0.07 (13.2 examples/sec; 0.380 sec/batch), lr: 0.009025
2016-08-28 03:28:51.921013: step 6180/9000 (epoch 35/50), acc = 1.00, loss = 0.02 (11.3 examples/sec; 0.442 sec/batch), lr: 0.009025
2016-08-28 03:29:00.038322: step 6190/9000 (epoch 35/50), acc = 1.00, loss = 0.03 (12.9 examples/sec; 0.389 sec/batch), lr: 0.009025
2016-08-28 03:29:07.615153: step 6200/9000 (epoch 35/50), acc = 1.00, loss = 0.01 (11.3 examples/sec; 0.444 sec/batch), lr: 0.009025

Step 6200: train_loss = 0.034629, train_accuracy = 1.000
Step 6200: dev_loss = 0.389170, dev_accuracy = 0.840

2016-08-28 03:29:29.190517: step 6210/9000 (epoch 35/50), acc = 1.00, loss = 0.04 (7.1 examples/sec; 0.708 sec/batch), lr: 0.009025
2016-08-28 03:29:36.701296: step 6220/9000 (epoch 35/50), acc = 1.00, loss = 0.04 (13.4 examples/sec; 0.373 sec/batch), lr: 0.009025
2016-08-28 03:29:44.858244: step 6230/9000 (epoch 35/50), acc = 1.00, loss = 0.08 (12.2 examples/sec; 0.410 sec/batch), lr: 0.009025
2016-08-28 03:29:52.457114: step 6240/9000 (epoch 35/50), acc = 1.00, loss = 0.05 (11.6 examples/sec; 0.429 sec/batch), lr: 0.009025
2016-08-28 03:29:59.619216: step 6250/9000 (epoch 35/50), acc = 1.00, loss = 0.01 (13.4 examples/sec; 0.373 sec/batch), lr: 0.009025

Step 6250: train_loss = 0.035697, train_accuracy = 1.000
Step 6250: dev_loss = 0.375819, dev_accuracy = 0.830

2016-08-28 03:30:23.842625: step 6260/9000 (epoch 35/50), acc = 1.00, loss = 0.01 (10.9 examples/sec; 0.458 sec/batch), lr: 0.009025
2016-08-28 03:30:31.188061: step 6270/9000 (epoch 35/50), acc = 1.00, loss = 0.05 (12.8 examples/sec; 0.391 sec/batch), lr: 0.009025
2016-08-28 03:30:39.654561: step 6280/9000 (epoch 35/50), acc = 1.00, loss = 0.02 (10.1 examples/sec; 0.497 sec/batch), lr: 0.009025
2016-08-28 03:30:47.342358: step 6290/9000 (epoch 35/50), acc = 1.00, loss = 0.02 (11.3 examples/sec; 0.444 sec/batch), lr: 0.009025
2016-08-28 03:30:54.906180: step 6300/9000 (epoch 35/50), acc = 1.00, loss = 0.02 (14.0 examples/sec; 0.358 sec/batch), lr: 0.009025

Step 6300: train_loss = 0.035900, train_accuracy = 1.000
Step 6300: dev_loss = 0.371137, dev_accuracy = 0.810

2016-08-28 03:31:15.884727: step 6310/9000 (epoch 36/50), acc = 1.00, loss = 0.05 (14.3 examples/sec; 0.349 sec/batch), lr: 0.009025
2016-08-28 03:31:22.796436: step 6320/9000 (epoch 36/50), acc = 1.00, loss = 0.03 (13.5 examples/sec; 0.369 sec/batch), lr: 0.009025
2016-08-28 03:31:30.192630: step 6330/9000 (epoch 36/50), acc = 1.00, loss = 0.04 (13.3 examples/sec; 0.377 sec/batch), lr: 0.009025
2016-08-28 03:31:37.214632: step 6340/9000 (epoch 36/50), acc = 1.00, loss = 0.03 (14.5 examples/sec; 0.345 sec/batch), lr: 0.009025
2016-08-28 03:31:44.222506: step 6350/9000 (epoch 36/50), acc = 1.00, loss = 0.05 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009025

Step 6350: train_loss = 0.037521, train_accuracy = 1.000
Step 6350: dev_loss = 0.406455, dev_accuracy = 0.830

2016-08-28 03:32:05.343027: step 6360/9000 (epoch 36/50), acc = 1.00, loss = 0.00 (13.2 examples/sec; 0.378 sec/batch), lr: 0.009025
2016-08-28 03:32:12.516691: step 6370/9000 (epoch 36/50), acc = 1.00, loss = 0.02 (14.2 examples/sec; 0.353 sec/batch), lr: 0.009025
2016-08-28 03:32:19.366525: step 6380/9000 (epoch 36/50), acc = 1.00, loss = 0.03 (14.4 examples/sec; 0.347 sec/batch), lr: 0.009025
2016-08-28 03:32:26.716728: step 6390/9000 (epoch 36/50), acc = 1.00, loss = 0.03 (14.6 examples/sec; 0.343 sec/batch), lr: 0.009025
2016-08-28 03:32:33.727465: step 6400/9000 (epoch 36/50), acc = 1.00, loss = 0.02 (14.3 examples/sec; 0.349 sec/batch), lr: 0.009025

Step 6400: train_loss = 0.033263, train_accuracy = 1.000
Step 6400: dev_loss = 0.380309, dev_accuracy = 0.840

2016-08-28 03:32:54.830641: step 6410/9000 (epoch 36/50), acc = 1.00, loss = 0.02 (14.4 examples/sec; 0.348 sec/batch), lr: 0.009025
2016-08-28 03:33:02.227286: step 6420/9000 (epoch 36/50), acc = 1.00, loss = 0.05 (13.3 examples/sec; 0.375 sec/batch), lr: 0.009025
2016-08-28 03:33:09.570724: step 6430/9000 (epoch 36/50), acc = 1.00, loss = 0.03 (13.9 examples/sec; 0.358 sec/batch), lr: 0.009025
2016-08-28 03:33:16.581012: step 6440/9000 (epoch 36/50), acc = 1.00, loss = 0.03 (13.6 examples/sec; 0.369 sec/batch), lr: 0.009025
2016-08-28 03:33:23.711051: step 6450/9000 (epoch 36/50), acc = 1.00, loss = 0.02 (14.4 examples/sec; 0.348 sec/batch), lr: 0.009025

Step 6450: train_loss = 0.033200, train_accuracy = 1.000
Step 6450: dev_loss = 0.369990, dev_accuracy = 0.820

2016-08-28 03:33:46.285938: step 6460/9000 (epoch 36/50), acc = 1.00, loss = 0.03 (12.4 examples/sec; 0.405 sec/batch), lr: 0.009025
2016-08-28 03:33:54.056825: step 6470/9000 (epoch 36/50), acc = 1.00, loss = 0.02 (11.4 examples/sec; 0.438 sec/batch), lr: 0.009025
2016-08-28 03:34:01.891068: step 6480/9000 (epoch 36/50), acc = 1.00, loss = 0.03 (10.0 examples/sec; 0.500 sec/batch), lr: 0.009025
2016-08-28 03:34:09.639908: step 6490/9000 (epoch 37/50), acc = 1.00, loss = 0.02 (13.8 examples/sec; 0.363 sec/batch), lr: 0.009025
2016-08-28 03:34:16.709987: step 6500/9000 (epoch 37/50), acc = 1.00, loss = 0.04 (14.0 examples/sec; 0.357 sec/batch), lr: 0.009025

Step 6500: train_loss = 0.027220, train_accuracy = 1.000
Step 6500: dev_loss = 0.371123, dev_accuracy = 0.830

2016-08-28 03:34:38.989892: step 6510/9000 (epoch 37/50), acc = 1.00, loss = 0.04 (12.5 examples/sec; 0.399 sec/batch), lr: 0.009025
2016-08-28 03:34:46.283424: step 6520/9000 (epoch 37/50), acc = 1.00, loss = 0.02 (14.4 examples/sec; 0.348 sec/batch), lr: 0.009025
2016-08-28 03:34:53.467101: step 6530/9000 (epoch 37/50), acc = 1.00, loss = 0.09 (14.2 examples/sec; 0.353 sec/batch), lr: 0.009025
2016-08-28 03:35:00.798502: step 6540/9000 (epoch 37/50), acc = 1.00, loss = 0.01 (13.4 examples/sec; 0.373 sec/batch), lr: 0.009025
2016-08-28 03:35:08.089622: step 6550/9000 (epoch 37/50), acc = 1.00, loss = 0.03 (14.2 examples/sec; 0.351 sec/batch), lr: 0.009025

Step 6550: train_loss = 0.027956, train_accuracy = 1.000
Step 6550: dev_loss = 0.391363, dev_accuracy = 0.840

2016-08-28 03:35:29.889958: step 6560/9000 (epoch 37/50), acc = 1.00, loss = 0.02 (12.3 examples/sec; 0.405 sec/batch), lr: 0.009025
2016-08-28 03:35:37.175974: step 6570/9000 (epoch 37/50), acc = 1.00, loss = 0.03 (13.7 examples/sec; 0.366 sec/batch), lr: 0.009025
2016-08-28 03:35:44.411831: step 6580/9000 (epoch 37/50), acc = 1.00, loss = 0.02 (13.4 examples/sec; 0.374 sec/batch), lr: 0.009025
2016-08-28 03:35:52.008612: step 6590/9000 (epoch 37/50), acc = 1.00, loss = 0.01 (11.6 examples/sec; 0.432 sec/batch), lr: 0.009025
2016-08-28 03:35:59.421351: step 6600/9000 (epoch 37/50), acc = 1.00, loss = 0.05 (13.1 examples/sec; 0.383 sec/batch), lr: 0.009025

Step 6600: train_loss = 0.027011, train_accuracy = 1.000
Step 6600: dev_loss = 0.372282, dev_accuracy = 0.840

2016-08-28 03:36:21.466932: step 6610/9000 (epoch 37/50), acc = 1.00, loss = 0.01 (14.0 examples/sec; 0.358 sec/batch), lr: 0.009025
2016-08-28 03:36:29.306886: step 6620/9000 (epoch 37/50), acc = 1.00, loss = 0.01 (13.4 examples/sec; 0.372 sec/batch), lr: 0.009025
2016-08-28 03:36:37.174765: step 6630/9000 (epoch 37/50), acc = 1.00, loss = 0.06 (12.9 examples/sec; 0.388 sec/batch), lr: 0.009025
2016-08-28 03:36:45.253984: step 6640/9000 (epoch 37/50), acc = 1.00, loss = 0.02 (12.8 examples/sec; 0.391 sec/batch), lr: 0.009025
2016-08-28 03:36:53.263972: step 6650/9000 (epoch 37/50), acc = 1.00, loss = 0.03 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009025

Step 6650: train_loss = 0.030411, train_accuracy = 0.999
Step 6650: dev_loss = 0.368937, dev_accuracy = 0.820

2016-08-28 03:37:16.433179: step 6660/9000 (epoch 37/50), acc = 1.00, loss = 0.04 (14.2 examples/sec; 0.353 sec/batch), lr: 0.009025
2016-08-28 03:37:24.625957: step 6670/9000 (epoch 38/50), acc = 1.00, loss = 0.02 (12.0 examples/sec; 0.416 sec/batch), lr: 0.009025
2016-08-28 03:37:32.139957: step 6680/9000 (epoch 38/50), acc = 1.00, loss = 0.04 (14.0 examples/sec; 0.358 sec/batch), lr: 0.009025
2016-08-28 03:37:40.394061: step 6690/9000 (epoch 38/50), acc = 1.00, loss = 0.02 (10.1 examples/sec; 0.495 sec/batch), lr: 0.009025
2016-08-28 03:37:47.671805: step 6700/9000 (epoch 38/50), acc = 1.00, loss = 0.03 (13.9 examples/sec; 0.359 sec/batch), lr: 0.009025

Step 6700: train_loss = 0.037373, train_accuracy = 0.995
Step 6700: dev_loss = 0.392961, dev_accuracy = 0.830

2016-08-28 03:38:10.312207: step 6710/9000 (epoch 38/50), acc = 1.00, loss = 0.02 (14.3 examples/sec; 0.348 sec/batch), lr: 0.009025
2016-08-28 03:38:17.292584: step 6720/9000 (epoch 38/50), acc = 1.00, loss = 0.01 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009025
2016-08-28 03:38:24.362701: step 6730/9000 (epoch 38/50), acc = 1.00, loss = 0.04 (13.9 examples/sec; 0.360 sec/batch), lr: 0.009025
2016-08-28 03:38:31.443087: step 6740/9000 (epoch 38/50), acc = 1.00, loss = 0.07 (14.0 examples/sec; 0.357 sec/batch), lr: 0.009025
2016-08-28 03:38:38.732802: step 6750/9000 (epoch 38/50), acc = 1.00, loss = 0.04 (13.6 examples/sec; 0.367 sec/batch), lr: 0.009025

Step 6750: train_loss = 0.030957, train_accuracy = 0.998
Step 6750: dev_loss = 0.373676, dev_accuracy = 0.840

2016-08-28 03:39:01.339745: step 6760/9000 (epoch 38/50), acc = 1.00, loss = 0.01 (13.5 examples/sec; 0.371 sec/batch), lr: 0.009025
2016-08-28 03:39:09.084934: step 6770/9000 (epoch 38/50), acc = 1.00, loss = 0.03 (11.9 examples/sec; 0.421 sec/batch), lr: 0.009025
2016-08-28 03:39:17.173311: step 6780/9000 (epoch 38/50), acc = 1.00, loss = 0.04 (13.5 examples/sec; 0.370 sec/batch), lr: 0.009025
2016-08-28 03:39:25.175017: step 6790/9000 (epoch 38/50), acc = 1.00, loss = 0.02 (12.3 examples/sec; 0.405 sec/batch), lr: 0.009025
2016-08-28 03:39:32.883480: step 6800/9000 (epoch 38/50), acc = 1.00, loss = 0.03 (13.1 examples/sec; 0.382 sec/batch), lr: 0.009025

Step 6800: train_loss = 0.030727, train_accuracy = 0.999
Step 6800: dev_loss = 0.367742, dev_accuracy = 0.830

2016-08-28 03:39:57.465856: step 6810/9000 (epoch 38/50), acc = 1.00, loss = 0.05 (13.4 examples/sec; 0.373 sec/batch), lr: 0.009025
2016-08-28 03:40:05.818173: step 6820/9000 (epoch 38/50), acc = 1.00, loss = 0.02 (10.7 examples/sec; 0.469 sec/batch), lr: 0.009025
2016-08-28 03:40:14.469221: step 6830/9000 (epoch 38/50), acc = 1.00, loss = 0.02 (13.8 examples/sec; 0.361 sec/batch), lr: 0.009025
2016-08-28 03:40:22.459860: step 6840/9000 (epoch 38/50), acc = 1.00, loss = 0.03 (12.0 examples/sec; 0.418 sec/batch), lr: 0.009025
2016-08-28 03:40:30.280411: step 6850/9000 (epoch 39/50), acc = 1.00, loss = 0.01 (11.6 examples/sec; 0.432 sec/batch), lr: 0.009025

Step 6850: train_loss = 0.029528, train_accuracy = 1.000
Step 6850: dev_loss = 0.368090, dev_accuracy = 0.830

2016-08-28 03:40:54.060470: step 6860/9000 (epoch 39/50), acc = 1.00, loss = 0.02 (13.4 examples/sec; 0.372 sec/batch), lr: 0.009025
2016-08-28 03:41:01.586601: step 6870/9000 (epoch 39/50), acc = 1.00, loss = 0.04 (13.0 examples/sec; 0.386 sec/batch), lr: 0.009025
2016-08-28 03:41:09.561649: step 6880/9000 (epoch 39/50), acc = 1.00, loss = 0.02 (10.2 examples/sec; 0.491 sec/batch), lr: 0.009025
2016-08-28 03:41:18.206807: step 6890/9000 (epoch 39/50), acc = 1.00, loss = 0.01 (9.2 examples/sec; 0.543 sec/batch), lr: 0.009025
2016-08-28 03:41:26.770674: step 6900/9000 (epoch 39/50), acc = 1.00, loss = 0.02 (9.2 examples/sec; 0.541 sec/batch), lr: 0.009025

Step 6900: train_loss = 0.027706, train_accuracy = 1.000
Step 6900: dev_loss = 0.393174, dev_accuracy = 0.840

2016-08-28 03:41:50.153619: step 6910/9000 (epoch 39/50), acc = 1.00, loss = 0.02 (11.8 examples/sec; 0.423 sec/batch), lr: 0.009025
2016-08-28 03:41:58.092876: step 6920/9000 (epoch 39/50), acc = 1.00, loss = 0.01 (13.5 examples/sec; 0.371 sec/batch), lr: 0.009025
2016-08-28 03:42:05.848257: step 6930/9000 (epoch 39/50), acc = 1.00, loss = 0.01 (13.3 examples/sec; 0.375 sec/batch), lr: 0.009025
2016-08-28 03:42:13.692839: step 6940/9000 (epoch 39/50), acc = 1.00, loss = 0.02 (13.5 examples/sec; 0.372 sec/batch), lr: 0.009025
2016-08-28 03:42:21.426394: step 6950/9000 (epoch 39/50), acc = 1.00, loss = 0.05 (13.5 examples/sec; 0.371 sec/batch), lr: 0.009025

Step 6950: train_loss = 0.026464, train_accuracy = 1.000
Step 6950: dev_loss = 0.372422, dev_accuracy = 0.840

2016-08-28 03:42:47.541989: step 6960/9000 (epoch 39/50), acc = 1.00, loss = 0.08 (13.4 examples/sec; 0.373 sec/batch), lr: 0.009025
2016-08-28 03:42:55.651291: step 6970/9000 (epoch 39/50), acc = 1.00, loss = 0.02 (8.2 examples/sec; 0.608 sec/batch), lr: 0.009025
2016-08-28 03:43:04.051721: step 6980/9000 (epoch 39/50), acc = 1.00, loss = 0.02 (13.0 examples/sec; 0.385 sec/batch), lr: 0.009025
2016-08-28 03:43:12.697782: step 6990/9000 (epoch 39/50), acc = 1.00, loss = 0.04 (9.9 examples/sec; 0.506 sec/batch), lr: 0.009025
2016-08-28 03:43:21.268644: step 7000/9000 (epoch 39/50), acc = 1.00, loss = 0.02 (12.1 examples/sec; 0.415 sec/batch), lr: 0.009025

Step 7000: train_loss = 0.026605, train_accuracy = 1.000
Step 7000: dev_loss = 0.369498, dev_accuracy = 0.830

2016-08-28 03:43:46.737746: step 7010/9000 (epoch 39/50), acc = 1.00, loss = 0.01 (12.8 examples/sec; 0.391 sec/batch), lr: 0.009025
2016-08-28 03:43:54.853977: step 7020/9000 (epoch 39/50), acc = 1.00, loss = 0.01 (8.3 examples/sec; 0.604 sec/batch), lr: 0.009025
2016-08-28 03:44:02.902522: step 7030/9000 (epoch 40/50), acc = 1.00, loss = 0.02 (13.4 examples/sec; 0.373 sec/batch), lr: 0.009025
2016-08-28 03:44:10.679302: step 7040/9000 (epoch 40/50), acc = 1.00, loss = 0.01 (12.4 examples/sec; 0.403 sec/batch), lr: 0.009025
2016-08-28 03:44:19.789941: step 7050/9000 (epoch 40/50), acc = 1.00, loss = 0.03 (12.9 examples/sec; 0.389 sec/batch), lr: 0.009025

Step 7050: train_loss = 0.033504, train_accuracy = 1.000
Step 7050: dev_loss = 0.379389, dev_accuracy = 0.840

2016-08-28 03:44:42.455996: step 7060/9000 (epoch 40/50), acc = 1.00, loss = 0.04 (13.8 examples/sec; 0.363 sec/batch), lr: 0.009025
2016-08-28 03:44:50.142447: step 7070/9000 (epoch 40/50), acc = 1.00, loss = 0.01 (13.0 examples/sec; 0.384 sec/batch), lr: 0.009025
2016-08-28 03:44:57.541728: step 7080/9000 (epoch 40/50), acc = 1.00, loss = 0.01 (13.8 examples/sec; 0.364 sec/batch), lr: 0.009025
2016-08-28 03:45:05.126634: step 7090/9000 (epoch 40/50), acc = 1.00, loss = 0.02 (14.1 examples/sec; 0.355 sec/batch), lr: 0.009025
2016-08-28 03:45:12.373204: step 7100/9000 (epoch 40/50), acc = 1.00, loss = 0.01 (14.2 examples/sec; 0.353 sec/batch), lr: 0.009025

Step 7100: train_loss = 0.027639, train_accuracy = 1.000
Step 7100: dev_loss = 0.385033, dev_accuracy = 0.840

2016-08-28 03:45:34.283694: step 7110/9000 (epoch 40/50), acc = 1.00, loss = 0.04 (11.2 examples/sec; 0.448 sec/batch), lr: 0.009025
2016-08-28 03:45:41.837140: step 7120/9000 (epoch 40/50), acc = 1.00, loss = 0.03 (13.1 examples/sec; 0.381 sec/batch), lr: 0.009025
2016-08-28 03:45:49.092064: step 7130/9000 (epoch 40/50), acc = 1.00, loss = 0.04 (13.5 examples/sec; 0.369 sec/batch), lr: 0.009025
2016-08-28 03:45:56.190968: step 7140/9000 (epoch 40/50), acc = 1.00, loss = 0.04 (14.1 examples/sec; 0.356 sec/batch), lr: 0.009025
2016-08-28 03:46:03.792114: step 7150/9000 (epoch 40/50), acc = 1.00, loss = 0.02 (12.1 examples/sec; 0.414 sec/batch), lr: 0.009025

Step 7150: train_loss = 0.026771, train_accuracy = 1.000
Step 7150: dev_loss = 0.370847, dev_accuracy = 0.840

2016-08-28 03:46:27.023277: step 7160/9000 (epoch 40/50), acc = 1.00, loss = 0.01 (11.8 examples/sec; 0.423 sec/batch), lr: 0.009025
2016-08-28 03:46:35.563803: step 7170/9000 (epoch 40/50), acc = 1.00, loss = 0.06 (12.4 examples/sec; 0.404 sec/batch), lr: 0.009025
2016-08-28 03:46:42.864834: step 7180/9000 (epoch 40/50), acc = 1.00, loss = 0.01 (14.2 examples/sec; 0.353 sec/batch), lr: 0.009025
2016-08-28 03:46:50.278248: step 7190/9000 (epoch 40/50), acc = 1.00, loss = 0.03 (12.5 examples/sec; 0.399 sec/batch), lr: 0.009025
2016-08-28 03:46:57.908418: step 7200/9000 (epoch 40/50), acc = 1.00, loss = 0.01 (14.0 examples/sec; 0.357 sec/batch), lr: 0.009025

Step 7200: train_loss = 0.028056, train_accuracy = 1.000
Step 7200: dev_loss = 0.367059, dev_accuracy = 0.830

2016-08-28 03:47:20.726727: step 7210/9000 (epoch 41/50), acc = 1.00, loss = 0.10 (12.1 examples/sec; 0.412 sec/batch), lr: 0.009025
2016-08-28 03:47:25.564306: step 7216/9000 (epoch 41/50), LR decays to 0.00857
2016-08-28 03:47:30.096837: step 7220/9000 (epoch 41/50), acc = 1.00, loss = 0.03 (12.3 examples/sec; 0.405 sec/batch), lr: 0.008574
2016-08-28 03:47:38.959552: step 7230/9000 (epoch 41/50), acc = 1.00, loss = 0.01 (13.8 examples/sec; 0.361 sec/batch), lr: 0.008574
2016-08-28 03:47:46.948722: step 7240/9000 (epoch 41/50), acc = 1.00, loss = 0.03 (11.8 examples/sec; 0.425 sec/batch), lr: 0.008574
2016-08-28 03:47:54.710739: step 7250/9000 (epoch 41/50), acc = 1.00, loss = 0.02 (14.3 examples/sec; 0.350 sec/batch), lr: 0.008574

Step 7250: train_loss = 0.030608, train_accuracy = 1.000
Step 7250: dev_loss = 0.395956, dev_accuracy = 0.830

2016-08-28 03:48:17.333377: step 7260/9000 (epoch 41/50), acc = 1.00, loss = 0.01 (13.7 examples/sec; 0.365 sec/batch), lr: 0.008574
2016-08-28 03:48:25.353095: step 7270/9000 (epoch 41/50), acc = 1.00, loss = 0.01 (12.0 examples/sec; 0.416 sec/batch), lr: 0.008574
2016-08-28 03:48:33.795830: step 7280/9000 (epoch 41/50), acc = 1.00, loss = 0.01 (10.1 examples/sec; 0.493 sec/batch), lr: 0.008574
2016-08-28 03:48:42.628927: step 7290/9000 (epoch 41/50), acc = 1.00, loss = 0.02 (12.5 examples/sec; 0.400 sec/batch), lr: 0.008574
2016-08-28 03:48:50.837810: step 7300/9000 (epoch 41/50), acc = 1.00, loss = 0.03 (12.4 examples/sec; 0.404 sec/batch), lr: 0.008574

Step 7300: train_loss = 0.026288, train_accuracy = 1.000
Step 7300: dev_loss = 0.374597, dev_accuracy = 0.840

2016-08-28 03:49:17.344195: step 7310/9000 (epoch 41/50), acc = 1.00, loss = 0.02 (11.9 examples/sec; 0.419 sec/batch), lr: 0.008574
2016-08-28 03:49:25.796704: step 7320/9000 (epoch 41/50), acc = 1.00, loss = 0.16 (12.7 examples/sec; 0.393 sec/batch), lr: 0.008574
2016-08-28 03:49:33.899250: step 7330/9000 (epoch 41/50), acc = 1.00, loss = 0.05 (13.6 examples/sec; 0.366 sec/batch), lr: 0.008574
2016-08-28 03:49:42.325237: step 7340/9000 (epoch 41/50), acc = 1.00, loss = 0.02 (11.6 examples/sec; 0.432 sec/batch), lr: 0.008574
2016-08-28 03:49:50.874174: step 7350/9000 (epoch 41/50), acc = 1.00, loss = 0.02 (10.7 examples/sec; 0.466 sec/batch), lr: 0.008574

Step 7350: train_loss = 0.027619, train_accuracy = 1.000
Step 7350: dev_loss = 0.366083, dev_accuracy = 0.850

2016-08-28 03:50:17.142489: step 7360/9000 (epoch 41/50), acc = 1.00, loss = 0.02 (7.8 examples/sec; 0.639 sec/batch), lr: 0.008574
2016-08-28 03:50:25.859537: step 7370/9000 (epoch 41/50), acc = 1.00, loss = 0.02 (11.3 examples/sec; 0.443 sec/batch), lr: 0.008574
2016-08-28 03:50:33.445089: step 7380/9000 (epoch 41/50), acc = 1.00, loss = 0.03 (11.9 examples/sec; 0.419 sec/batch), lr: 0.008574
2016-08-28 03:50:41.226320: step 7390/9000 (epoch 42/50), acc = 1.00, loss = 0.01 (13.3 examples/sec; 0.377 sec/batch), lr: 0.008574
2016-08-28 03:50:48.947947: step 7400/9000 (epoch 42/50), acc = 1.00, loss = 0.01 (13.7 examples/sec; 0.366 sec/batch), lr: 0.008574

Step 7400: train_loss = 0.025873, train_accuracy = 1.000
Step 7400: dev_loss = 0.371484, dev_accuracy = 0.840

2016-08-28 03:51:12.509589: step 7410/9000 (epoch 42/50), acc = 1.00, loss = 0.03 (14.3 examples/sec; 0.349 sec/batch), lr: 0.008574
2016-08-28 03:51:20.265080: step 7420/9000 (epoch 42/50), acc = 1.00, loss = 0.03 (12.2 examples/sec; 0.409 sec/batch), lr: 0.008574
2016-08-28 03:51:28.213782: step 7430/9000 (epoch 42/50), acc = 1.00, loss = 0.01 (14.0 examples/sec; 0.358 sec/batch), lr: 0.008574
2016-08-28 03:51:35.791485: step 7440/9000 (epoch 42/50), acc = 1.00, loss = 0.01 (13.6 examples/sec; 0.368 sec/batch), lr: 0.008574
2016-08-28 03:51:43.495559: step 7450/9000 (epoch 42/50), acc = 1.00, loss = 0.02 (13.6 examples/sec; 0.368 sec/batch), lr: 0.008574

Step 7450: train_loss = 0.025581, train_accuracy = 1.000
Step 7450: dev_loss = 0.404904, dev_accuracy = 0.830

2016-08-28 03:52:06.408976: step 7460/9000 (epoch 42/50), acc = 1.00, loss = 0.02 (14.0 examples/sec; 0.357 sec/batch), lr: 0.008574
2016-08-28 03:52:13.748290: step 7470/9000 (epoch 42/50), acc = 1.00, loss = 0.02 (13.4 examples/sec; 0.374 sec/batch), lr: 0.008574
2016-08-28 03:52:21.712580: step 7480/9000 (epoch 42/50), acc = 1.00, loss = 0.01 (12.1 examples/sec; 0.412 sec/batch), lr: 0.008574
2016-08-28 03:52:29.536689: step 7490/9000 (epoch 42/50), acc = 1.00, loss = 0.03 (13.7 examples/sec; 0.364 sec/batch), lr: 0.008574
2016-08-28 03:52:37.278682: step 7500/9000 (epoch 42/50), acc = 1.00, loss = 0.03 (14.0 examples/sec; 0.357 sec/batch), lr: 0.008574

Step 7500: train_loss = 0.024295, train_accuracy = 1.000
Step 7500: dev_loss = 0.370732, dev_accuracy = 0.830

2016-08-28 03:53:03.882974: step 7510/9000 (epoch 42/50), acc = 1.00, loss = 0.03 (10.9 examples/sec; 0.459 sec/batch), lr: 0.008574
2016-08-28 03:53:12.972530: step 7520/9000 (epoch 42/50), acc = 1.00, loss = 0.01 (11.2 examples/sec; 0.447 sec/batch), lr: 0.008574
2016-08-28 03:53:22.618692: step 7530/9000 (epoch 42/50), acc = 1.00, loss = 0.05 (13.6 examples/sec; 0.367 sec/batch), lr: 0.008574
2016-08-28 03:53:30.225015: step 7540/9000 (epoch 42/50), acc = 1.00, loss = 0.04 (14.2 examples/sec; 0.353 sec/batch), lr: 0.008574
2016-08-28 03:53:37.841676: step 7550/9000 (epoch 42/50), acc = 1.00, loss = 0.01 (12.6 examples/sec; 0.396 sec/batch), lr: 0.008574

Step 7550: train_loss = 0.026151, train_accuracy = 1.000
Step 7550: dev_loss = 0.364963, dev_accuracy = 0.830

2016-08-28 03:54:02.373066: step 7560/9000 (epoch 42/50), acc = 1.00, loss = 0.03 (12.9 examples/sec; 0.388 sec/batch), lr: 0.008574
2016-08-28 03:54:10.824389: step 7570/9000 (epoch 43/50), acc = 1.00, loss = 0.01 (11.1 examples/sec; 0.449 sec/batch), lr: 0.008574
2016-08-28 03:54:19.022813: step 7580/9000 (epoch 43/50), acc = 1.00, loss = 0.04 (14.1 examples/sec; 0.355 sec/batch), lr: 0.008574
2016-08-28 03:54:27.152717: step 7590/9000 (epoch 43/50), acc = 1.00, loss = 0.02 (13.9 examples/sec; 0.360 sec/batch), lr: 0.008574
2016-08-28 03:54:34.855579: step 7600/9000 (epoch 43/50), acc = 1.00, loss = 0.05 (12.2 examples/sec; 0.410 sec/batch), lr: 0.008574

Step 7600: train_loss = 0.024219, train_accuracy = 1.000
Step 7600: dev_loss = 0.380874, dev_accuracy = 0.840

2016-08-28 03:55:00.801854: step 7610/9000 (epoch 43/50), acc = 1.00, loss = 0.02 (12.1 examples/sec; 0.413 sec/batch), lr: 0.008574
2016-08-28 03:55:08.852474: step 7620/9000 (epoch 43/50), acc = 1.00, loss = 0.01 (14.2 examples/sec; 0.353 sec/batch), lr: 0.008574
2016-08-28 03:55:17.435107: step 7630/9000 (epoch 43/50), acc = 1.00, loss = 0.03 (11.7 examples/sec; 0.428 sec/batch), lr: 0.008574
2016-08-28 03:55:26.068626: step 7640/9000 (epoch 43/50), acc = 1.00, loss = 0.02 (13.4 examples/sec; 0.373 sec/batch), lr: 0.008574
2016-08-28 03:55:34.814795: step 7650/9000 (epoch 43/50), acc = 1.00, loss = 0.10 (12.2 examples/sec; 0.409 sec/batch), lr: 0.008574

Step 7650: train_loss = 0.023813, train_accuracy = 1.000
Step 7650: dev_loss = 0.375350, dev_accuracy = 0.840

2016-08-28 03:56:00.708787: step 7660/9000 (epoch 43/50), acc = 1.00, loss = 0.09 (12.7 examples/sec; 0.393 sec/batch), lr: 0.008574
2016-08-28 03:56:09.858256: step 7670/9000 (epoch 43/50), acc = 1.00, loss = 0.01 (13.6 examples/sec; 0.368 sec/batch), lr: 0.008574
2016-08-28 03:56:19.328142: step 7680/9000 (epoch 43/50), acc = 1.00, loss = 0.07 (12.1 examples/sec; 0.415 sec/batch), lr: 0.008574
2016-08-28 03:56:26.976206: step 7690/9000 (epoch 43/50), acc = 1.00, loss = 0.02 (14.7 examples/sec; 0.339 sec/batch), lr: 0.008574
2016-08-28 03:56:34.569336: step 7700/9000 (epoch 43/50), acc = 1.00, loss = 0.01 (14.1 examples/sec; 0.354 sec/batch), lr: 0.008574

Step 7700: train_loss = 0.024484, train_accuracy = 1.000
Step 7700: dev_loss = 0.365136, dev_accuracy = 0.830

2016-08-28 03:56:57.609887: step 7710/9000 (epoch 43/50), acc = 1.00, loss = 0.01 (14.1 examples/sec; 0.355 sec/batch), lr: 0.008574
2016-08-28 03:57:02.274097: step 7716/9000 (epoch 43/50), LR decays to 0.00815
2016-08-28 03:57:05.661573: step 7720/9000 (epoch 43/50), acc = 1.00, loss = 0.01 (10.1 examples/sec; 0.495 sec/batch), lr: 0.008145
2016-08-28 03:57:14.104211: step 7730/9000 (epoch 43/50), acc = 1.00, loss = 0.04 (13.6 examples/sec; 0.369 sec/batch), lr: 0.008145
2016-08-28 03:57:21.997098: step 7740/9000 (epoch 43/50), acc = 1.00, loss = 0.02 (13.7 examples/sec; 0.366 sec/batch), lr: 0.008145
2016-08-28 03:57:30.205316: step 7750/9000 (epoch 44/50), acc = 1.00, loss = 0.01 (12.9 examples/sec; 0.387 sec/batch), lr: 0.008145

Step 7750: train_loss = 0.026258, train_accuracy = 1.000
Step 7750: dev_loss = 0.364748, dev_accuracy = 0.830

2016-08-28 03:57:55.147235: step 7760/9000 (epoch 44/50), acc = 1.00, loss = 0.01 (14.7 examples/sec; 0.341 sec/batch), lr: 0.008145
2016-08-28 03:58:02.822858: step 7770/9000 (epoch 44/50), acc = 1.00, loss = 0.02 (13.6 examples/sec; 0.367 sec/batch), lr: 0.008145
2016-08-28 03:58:11.717775: step 7780/9000 (epoch 44/50), acc = 1.00, loss = 0.02 (12.7 examples/sec; 0.393 sec/batch), lr: 0.008145
2016-08-28 03:58:20.433847: step 7790/9000 (epoch 44/50), acc = 1.00, loss = 0.01 (11.4 examples/sec; 0.438 sec/batch), lr: 0.008145
2016-08-28 03:58:28.710546: step 7800/9000 (epoch 44/50), acc = 1.00, loss = 0.02 (13.9 examples/sec; 0.358 sec/batch), lr: 0.008145

Step 7800: train_loss = 0.021890, train_accuracy = 1.000
Step 7800: dev_loss = 0.383679, dev_accuracy = 0.850

2016-08-28 03:58:54.325221: step 7810/9000 (epoch 44/50), acc = 1.00, loss = 0.02 (12.4 examples/sec; 0.405 sec/batch), lr: 0.008145
2016-08-28 03:59:02.634018: step 7820/9000 (epoch 44/50), acc = 1.00, loss = 0.04 (13.5 examples/sec; 0.370 sec/batch), lr: 0.008145
2016-08-28 03:59:11.837110: step 7830/9000 (epoch 44/50), acc = 1.00, loss = 0.05 (13.1 examples/sec; 0.383 sec/batch), lr: 0.008145
2016-08-28 03:59:19.474346: step 7840/9000 (epoch 44/50), acc = 1.00, loss = 0.05 (13.9 examples/sec; 0.360 sec/batch), lr: 0.008145
2016-08-28 03:59:27.565250: step 7850/9000 (epoch 44/50), acc = 1.00, loss = 0.01 (8.2 examples/sec; 0.613 sec/batch), lr: 0.008145

Step 7850: train_loss = 0.023166, train_accuracy = 1.000
Step 7850: dev_loss = 0.371998, dev_accuracy = 0.840

2016-08-28 03:59:53.413067: step 7860/9000 (epoch 44/50), acc = 1.00, loss = 0.04 (9.3 examples/sec; 0.539 sec/batch), lr: 0.008145
2016-08-28 04:00:01.822653: step 7870/9000 (epoch 44/50), acc = 1.00, loss = 0.02 (13.5 examples/sec; 0.369 sec/batch), lr: 0.008145
2016-08-28 04:00:09.509095: step 7880/9000 (epoch 44/50), acc = 1.00, loss = 0.01 (13.8 examples/sec; 0.363 sec/batch), lr: 0.008145
2016-08-28 04:00:18.174780: step 7890/9000 (epoch 44/50), acc = 1.00, loss = 0.03 (14.6 examples/sec; 0.342 sec/batch), lr: 0.008145
2016-08-28 04:00:26.003623: step 7900/9000 (epoch 44/50), acc = 1.00, loss = 0.02 (14.8 examples/sec; 0.337 sec/batch), lr: 0.008145

Step 7900: train_loss = 0.024255, train_accuracy = 1.000
Step 7900: dev_loss = 0.363956, dev_accuracy = 0.820

2016-08-28 04:00:50.642205: step 7910/9000 (epoch 44/50), acc = 1.00, loss = 0.02 (14.0 examples/sec; 0.357 sec/batch), lr: 0.008145
2016-08-28 04:00:58.995633: step 7920/9000 (epoch 44/50), acc = 1.00, loss = 0.03 (10.6 examples/sec; 0.472 sec/batch), lr: 0.008145
2016-08-28 04:01:08.042981: step 7930/9000 (epoch 45/50), acc = 1.00, loss = 0.02 (11.9 examples/sec; 0.419 sec/batch), lr: 0.008145
2016-08-28 04:01:16.721916: step 7940/9000 (epoch 45/50), acc = 1.00, loss = 0.02 (11.0 examples/sec; 0.455 sec/batch), lr: 0.008145
2016-08-28 04:01:25.296865: step 7950/9000 (epoch 45/50), acc = 1.00, loss = 0.06 (9.2 examples/sec; 0.543 sec/batch), lr: 0.008145

Step 7950: train_loss = 0.029227, train_accuracy = 1.000
Step 7950: dev_loss = 0.368400, dev_accuracy = 0.840

2016-08-28 04:01:50.881771: step 7960/9000 (epoch 45/50), acc = 1.00, loss = 0.05 (13.3 examples/sec; 0.377 sec/batch), lr: 0.008145
2016-08-28 04:01:59.719476: step 7970/9000 (epoch 45/50), acc = 1.00, loss = 0.02 (14.3 examples/sec; 0.350 sec/batch), lr: 0.008145
2016-08-28 04:02:08.184054: step 7980/9000 (epoch 45/50), acc = 1.00, loss = 0.00 (13.2 examples/sec; 0.380 sec/batch), lr: 0.008145
2016-08-28 04:02:16.215023: step 7990/9000 (epoch 45/50), acc = 1.00, loss = 0.02 (7.9 examples/sec; 0.629 sec/batch), lr: 0.008145
2016-08-28 04:02:24.620913: step 8000/9000 (epoch 45/50), acc = 1.00, loss = 0.01 (14.0 examples/sec; 0.358 sec/batch), lr: 0.008145

Step 8000: train_loss = 0.024180, train_accuracy = 1.000
Step 8000: dev_loss = 0.372764, dev_accuracy = 0.840

2016-08-28 04:02:48.115481: step 8010/9000 (epoch 45/50), acc = 1.00, loss = 0.02 (13.9 examples/sec; 0.361 sec/batch), lr: 0.008145
2016-08-28 04:02:56.074115: step 8020/9000 (epoch 45/50), acc = 1.00, loss = 0.02 (14.7 examples/sec; 0.340 sec/batch), lr: 0.008145
2016-08-28 04:03:03.823973: step 8030/9000 (epoch 45/50), acc = 1.00, loss = 0.01 (11.7 examples/sec; 0.426 sec/batch), lr: 0.008145
2016-08-28 04:03:11.569767: step 8040/9000 (epoch 45/50), acc = 1.00, loss = 0.05 (14.4 examples/sec; 0.347 sec/batch), lr: 0.008145
2016-08-28 04:03:19.291039: step 8050/9000 (epoch 45/50), acc = 1.00, loss = 0.02 (12.7 examples/sec; 0.393 sec/batch), lr: 0.008145

Step 8050: train_loss = 0.025912, train_accuracy = 0.998
Step 8050: dev_loss = 0.376158, dev_accuracy = 0.840

2016-08-28 04:03:44.724674: step 8060/9000 (epoch 45/50), acc = 1.00, loss = 0.01 (12.0 examples/sec; 0.416 sec/batch), lr: 0.008145
2016-08-28 04:03:52.536415: step 8070/9000 (epoch 45/50), acc = 1.00, loss = 0.01 (14.3 examples/sec; 0.349 sec/batch), lr: 0.008145
2016-08-28 04:04:00.362522: step 8080/9000 (epoch 45/50), acc = 1.00, loss = 0.03 (13.6 examples/sec; 0.369 sec/batch), lr: 0.008145
2016-08-28 04:04:08.206962: step 8090/9000 (epoch 45/50), acc = 1.00, loss = 0.03 (15.1 examples/sec; 0.332 sec/batch), lr: 0.008145
2016-08-28 04:04:15.930102: step 8100/9000 (epoch 45/50), acc = 1.00, loss = 0.03 (12.7 examples/sec; 0.394 sec/batch), lr: 0.008145

Step 8100: train_loss = 0.025345, train_accuracy = 0.999
Step 8100: dev_loss = 0.364881, dev_accuracy = 0.820

2016-08-28 04:04:40.004201: step 8110/9000 (epoch 46/50), acc = 1.00, loss = 0.01 (15.4 examples/sec; 0.324 sec/batch), lr: 0.008145
2016-08-28 04:04:47.771898: step 8120/9000 (epoch 46/50), acc = 1.00, loss = 0.02 (14.2 examples/sec; 0.353 sec/batch), lr: 0.008145
2016-08-28 04:04:55.896787: step 8130/9000 (epoch 46/50), acc = 1.00, loss = 0.01 (13.6 examples/sec; 0.368 sec/batch), lr: 0.008145
2016-08-28 04:05:04.425957: step 8140/9000 (epoch 46/50), acc = 1.00, loss = 0.07 (12.9 examples/sec; 0.387 sec/batch), lr: 0.008145
2016-08-28 04:05:12.005833: step 8150/9000 (epoch 46/50), acc = 1.00, loss = 0.01 (14.7 examples/sec; 0.340 sec/batch), lr: 0.008145

Step 8150: train_loss = 0.024952, train_accuracy = 1.000
Step 8150: dev_loss = 0.395694, dev_accuracy = 0.830

2016-08-28 04:05:35.009626: step 8160/9000 (epoch 46/50), acc = 1.00, loss = 0.01 (15.0 examples/sec; 0.332 sec/batch), lr: 0.008145
2016-08-28 04:05:42.885345: step 8170/9000 (epoch 46/50), acc = 1.00, loss = 0.02 (14.8 examples/sec; 0.338 sec/batch), lr: 0.008145
2016-08-28 04:05:50.917795: step 8180/9000 (epoch 46/50), acc = 1.00, loss = 0.02 (11.8 examples/sec; 0.424 sec/batch), lr: 0.008145
2016-08-28 04:05:58.614161: step 8190/9000 (epoch 46/50), acc = 1.00, loss = 0.02 (14.3 examples/sec; 0.351 sec/batch), lr: 0.008145
2016-08-28 04:06:06.229105: step 8200/9000 (epoch 46/50), acc = 1.00, loss = 0.03 (13.8 examples/sec; 0.362 sec/batch), lr: 0.008145

Step 8200: train_loss = 0.023969, train_accuracy = 1.000
Step 8200: dev_loss = 0.372130, dev_accuracy = 0.840

2016-08-28 04:06:28.599504: step 8210/9000 (epoch 46/50), acc = 1.00, loss = 0.01 (14.9 examples/sec; 0.335 sec/batch), lr: 0.008145
2016-08-28 04:06:35.889122: step 8220/9000 (epoch 46/50), acc = 1.00, loss = 0.02 (15.3 examples/sec; 0.327 sec/batch), lr: 0.008145
2016-08-28 04:06:43.174174: step 8230/9000 (epoch 46/50), acc = 1.00, loss = 0.01 (14.4 examples/sec; 0.348 sec/batch), lr: 0.008145
2016-08-28 04:06:50.641725: step 8240/9000 (epoch 46/50), acc = 1.00, loss = 0.02 (15.2 examples/sec; 0.328 sec/batch), lr: 0.008145
2016-08-28 04:06:58.084110: step 8250/9000 (epoch 46/50), acc = 1.00, loss = 0.06 (13.0 examples/sec; 0.383 sec/batch), lr: 0.008145

Step 8250: train_loss = 0.023254, train_accuracy = 1.000
Step 8250: dev_loss = 0.363990, dev_accuracy = 0.820

2016-08-28 04:07:22.917480: step 8260/9000 (epoch 46/50), acc = 1.00, loss = 0.05 (15.2 examples/sec; 0.329 sec/batch), lr: 0.008145
2016-08-28 04:07:30.387641: step 8270/9000 (epoch 46/50), acc = 1.00, loss = 0.01 (14.7 examples/sec; 0.340 sec/batch), lr: 0.008145
2016-08-28 04:07:37.915978: step 8280/9000 (epoch 46/50), acc = 1.00, loss = 0.03 (15.1 examples/sec; 0.331 sec/batch), lr: 0.008145
2016-08-28 04:07:45.426908: step 8290/9000 (epoch 47/50), acc = 1.00, loss = 0.02 (15.5 examples/sec; 0.324 sec/batch), lr: 0.008145
2016-08-28 04:07:52.941288: step 8300/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (14.2 examples/sec; 0.352 sec/batch), lr: 0.008145

Step 8300: train_loss = 0.024475, train_accuracy = 1.000
Step 8300: dev_loss = 0.367178, dev_accuracy = 0.840

2016-08-28 04:08:17.278150: step 8310/9000 (epoch 47/50), acc = 1.00, loss = 0.02 (13.7 examples/sec; 0.364 sec/batch), lr: 0.008145
2016-08-28 04:08:24.819814: step 8320/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (14.7 examples/sec; 0.339 sec/batch), lr: 0.008145
2016-08-28 04:08:32.592480: step 8330/9000 (epoch 47/50), acc = 1.00, loss = 0.02 (13.9 examples/sec; 0.360 sec/batch), lr: 0.008145
2016-08-28 04:08:40.035192: step 8340/9000 (epoch 47/50), acc = 1.00, loss = 0.02 (14.5 examples/sec; 0.345 sec/batch), lr: 0.008145
2016-08-28 04:08:47.567145: step 8350/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (14.9 examples/sec; 0.336 sec/batch), lr: 0.008145

Step 8350: train_loss = 0.022934, train_accuracy = 1.000
Step 8350: dev_loss = 0.387021, dev_accuracy = 0.840

2016-08-28 04:09:11.108507: step 8360/9000 (epoch 47/50), acc = 1.00, loss = 0.00 (14.7 examples/sec; 0.339 sec/batch), lr: 0.008145
2016-08-28 04:09:18.446544: step 8370/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (13.3 examples/sec; 0.375 sec/batch), lr: 0.008145
2016-08-28 04:09:25.968015: step 8380/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (14.8 examples/sec; 0.339 sec/batch), lr: 0.008145
2016-08-28 04:09:33.366491: step 8390/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (13.2 examples/sec; 0.378 sec/batch), lr: 0.008145
2016-08-28 04:09:40.830940: step 8400/9000 (epoch 47/50), acc = 1.00, loss = 0.03 (14.9 examples/sec; 0.337 sec/batch), lr: 0.008145

Step 8400: train_loss = 0.022417, train_accuracy = 1.000
Step 8400: dev_loss = 0.375216, dev_accuracy = 0.840

2016-08-28 04:10:06.508744: step 8410/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (12.4 examples/sec; 0.403 sec/batch), lr: 0.008145
2016-08-28 04:10:14.540520: step 8420/9000 (epoch 47/50), acc = 1.00, loss = 0.02 (13.8 examples/sec; 0.362 sec/batch), lr: 0.008145
2016-08-28 04:10:23.318251: step 8430/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (10.5 examples/sec; 0.475 sec/batch), lr: 0.008145
2016-08-28 04:10:31.044024: step 8440/9000 (epoch 47/50), acc = 1.00, loss = 0.04 (15.1 examples/sec; 0.332 sec/batch), lr: 0.008145
2016-08-28 04:10:38.349809: step 8450/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (15.2 examples/sec; 0.328 sec/batch), lr: 0.008145

Step 8450: train_loss = 0.022558, train_accuracy = 1.000
Step 8450: dev_loss = 0.363497, dev_accuracy = 0.820

2016-08-28 04:11:01.389322: step 8460/9000 (epoch 47/50), acc = 1.00, loss = 0.01 (15.2 examples/sec; 0.329 sec/batch), lr: 0.008145
2016-08-28 04:11:08.648035: step 8470/9000 (epoch 48/50), acc = 1.00, loss = 0.02 (15.3 examples/sec; 0.326 sec/batch), lr: 0.008145
2016-08-28 04:11:16.152451: step 8480/9000 (epoch 48/50), acc = 1.00, loss = 0.01 (15.0 examples/sec; 0.334 sec/batch), lr: 0.008145
2016-08-28 04:11:16.891590: step 8481/9000 (epoch 48/50), LR decays to 0.00774
2016-08-28 04:11:23.967133: step 8490/9000 (epoch 48/50), acc = 1.00, loss = 0.01 (15.0 examples/sec; 0.333 sec/batch), lr: 0.007738
2016-08-28 04:11:31.392862: step 8500/9000 (epoch 48/50), acc = 1.00, loss = 0.02 (13.9 examples/sec; 0.359 sec/batch), lr: 0.007738

Step 8500: train_loss = 0.023485, train_accuracy = 1.000
Step 8500: dev_loss = 0.374243, dev_accuracy = 0.840

2016-08-28 04:11:56.444369: step 8510/9000 (epoch 48/50), acc = 1.00, loss = 0.02 (14.6 examples/sec; 0.342 sec/batch), lr: 0.007738
2016-08-28 04:12:03.988725: step 8520/9000 (epoch 48/50), acc = 1.00, loss = 0.02 (14.8 examples/sec; 0.339 sec/batch), lr: 0.007738
2016-08-28 04:12:11.464535: step 8530/9000 (epoch 48/50), acc = 1.00, loss = 0.01 (14.1 examples/sec; 0.355 sec/batch), lr: 0.007738
2016-08-28 04:12:19.744303: step 8540/9000 (epoch 48/50), acc = 1.00, loss = 0.01 (13.8 examples/sec; 0.362 sec/batch), lr: 0.007738
2016-08-28 04:12:27.305308: step 8550/9000 (epoch 48/50), acc = 1.00, loss = 0.01 (14.9 examples/sec; 0.335 sec/batch), lr: 0.007738

Step 8550: train_loss = 0.021434, train_accuracy = 1.000
Step 8550: dev_loss = 0.372195, dev_accuracy = 0.840

2016-08-28 04:12:51.906319: step 8560/9000 (epoch 48/50), acc = 1.00, loss = 0.09 (14.3 examples/sec; 0.351 sec/batch), lr: 0.007738
2016-08-28 04:13:00.021040: step 8570/9000 (epoch 48/50), acc = 1.00, loss = 0.01 (15.2 examples/sec; 0.330 sec/batch), lr: 0.007738
2016-08-28 04:13:07.525592: step 8580/9000 (epoch 48/50), acc = 1.00, loss = 0.06 (15.0 examples/sec; 0.334 sec/batch), lr: 0.007738
2016-08-28 04:13:15.007715: step 8590/9000 (epoch 48/50), acc = 1.00, loss = 0.02 (14.2 examples/sec; 0.351 sec/batch), lr: 0.007738
2016-08-28 04:13:23.526395: step 8600/9000 (epoch 48/50), acc = 1.00, loss = 0.01 (13.9 examples/sec; 0.361 sec/batch), lr: 0.007738

Step 8600: train_loss = 0.021711, train_accuracy = 1.000
Step 8600: dev_loss = 0.370338, dev_accuracy = 0.830

2016-08-28 04:13:48.570686: step 8610/9000 (epoch 48/50), acc = 1.00, loss = 0.02 (14.8 examples/sec; 0.338 sec/batch), lr: 0.007738
2016-08-28 04:13:56.012425: step 8620/9000 (epoch 48/50), acc = 1.00, loss = 0.03 (15.0 examples/sec; 0.334 sec/batch), lr: 0.007738
2016-08-28 04:14:03.672963: step 8630/9000 (epoch 48/50), acc = 1.00, loss = 0.03 (14.7 examples/sec; 0.340 sec/batch), lr: 0.007738
2016-08-28 04:14:11.270420: step 8640/9000 (epoch 48/50), acc = 1.00, loss = 0.02 (15.0 examples/sec; 0.333 sec/batch), lr: 0.007738
2016-08-28 04:14:19.026359: step 8650/9000 (epoch 49/50), acc = 1.00, loss = 0.00 (13.7 examples/sec; 0.364 sec/batch), lr: 0.007738

Step 8650: train_loss = 0.019700, train_accuracy = 1.000
Step 8650: dev_loss = 0.365722, dev_accuracy = 0.840

2016-08-28 04:14:42.903481: step 8660/9000 (epoch 49/50), acc = 1.00, loss = 0.02 (15.2 examples/sec; 0.330 sec/batch), lr: 0.007738
2016-08-28 04:14:50.467491: step 8670/9000 (epoch 49/50), acc = 1.00, loss = 0.01 (14.8 examples/sec; 0.337 sec/batch), lr: 0.007738
2016-08-28 04:14:57.997709: step 8680/9000 (epoch 49/50), acc = 1.00, loss = 0.02 (15.1 examples/sec; 0.332 sec/batch), lr: 0.007738
2016-08-28 04:15:05.497782: step 8690/9000 (epoch 49/50), acc = 1.00, loss = 0.03 (15.2 examples/sec; 0.328 sec/batch), lr: 0.007738
2016-08-28 04:15:13.029151: step 8700/9000 (epoch 49/50), acc = 1.00, loss = 0.01 (15.4 examples/sec; 0.325 sec/batch), lr: 0.007738

Step 8700: train_loss = 0.021966, train_accuracy = 1.000
Step 8700: dev_loss = 0.387467, dev_accuracy = 0.840

2016-08-28 04:15:37.008182: step 8710/9000 (epoch 49/50), acc = 1.00, loss = 0.01 (15.0 examples/sec; 0.334 sec/batch), lr: 0.007738
2016-08-28 04:15:44.415324: step 8720/9000 (epoch 49/50), acc = 1.00, loss = 0.02 (14.9 examples/sec; 0.335 sec/batch), lr: 0.007738
2016-08-28 04:15:51.855672: step 8730/9000 (epoch 49/50), acc = 1.00, loss = 0.02 (15.0 examples/sec; 0.333 sec/batch), lr: 0.007738
2016-08-28 04:15:59.253656: step 8740/9000 (epoch 49/50), acc = 1.00, loss = 0.01 (14.8 examples/sec; 0.339 sec/batch), lr: 0.007738
2016-08-28 04:16:06.895137: step 8750/9000 (epoch 49/50), acc = 1.00, loss = 0.03 (15.1 examples/sec; 0.332 sec/batch), lr: 0.007738

Step 8750: train_loss = 0.022633, train_accuracy = 1.000
Step 8750: dev_loss = 0.371303, dev_accuracy = 0.840

2016-08-28 04:16:31.642688: step 8760/9000 (epoch 49/50), acc = 1.00, loss = 0.02 (15.0 examples/sec; 0.334 sec/batch), lr: 0.007738
2016-08-28 04:16:39.218820: step 8770/9000 (epoch 49/50), acc = 1.00, loss = 0.01 (14.1 examples/sec; 0.355 sec/batch), lr: 0.007738
2016-08-28 04:16:47.748824: step 8780/9000 (epoch 49/50), acc = 1.00, loss = 0.01 (14.9 examples/sec; 0.336 sec/batch), lr: 0.007738
2016-08-28 04:16:55.249992: step 8790/9000 (epoch 49/50), acc = 1.00, loss = 0.10 (15.0 examples/sec; 0.334 sec/batch), lr: 0.007738
2016-08-28 04:17:02.773284: step 8800/9000 (epoch 49/50), acc = 1.00, loss = 0.01 (15.4 examples/sec; 0.325 sec/batch), lr: 0.007738

Step 8800: train_loss = 0.023718, train_accuracy = 1.000
Step 8800: dev_loss = 0.366107, dev_accuracy = 0.840

2016-08-28 04:17:27.820702: step 8810/9000 (epoch 49/50), acc = 1.00, loss = 0.01 (14.1 examples/sec; 0.355 sec/batch), lr: 0.007738
2016-08-28 04:17:35.783899: step 8820/9000 (epoch 49/50), acc = 1.00, loss = 0.02 (11.6 examples/sec; 0.431 sec/batch), lr: 0.007738
2016-08-28 04:17:43.343475: step 8830/9000 (epoch 50/50), acc = 1.00, loss = 0.02 (15.2 examples/sec; 0.329 sec/batch), lr: 0.007738
2016-08-28 04:17:51.022663: step 8840/9000 (epoch 50/50), acc = 1.00, loss = 0.03 (15.1 examples/sec; 0.331 sec/batch), lr: 0.007738
2016-08-28 04:17:58.613731: step 8850/9000 (epoch 50/50), acc = 1.00, loss = 0.06 (14.7 examples/sec; 0.339 sec/batch), lr: 0.007738

Step 8850: train_loss = 0.021735, train_accuracy = 1.000
Step 8850: dev_loss = 0.369192, dev_accuracy = 0.830

2016-08-28 04:18:24.548325: step 8860/9000 (epoch 50/50), acc = 1.00, loss = 0.02 (14.7 examples/sec; 0.340 sec/batch), lr: 0.007738
2016-08-28 04:18:32.485555: step 8870/9000 (epoch 50/50), acc = 1.00, loss = 0.02 (11.9 examples/sec; 0.419 sec/batch), lr: 0.007738
2016-08-28 04:18:40.147631: step 8880/9000 (epoch 50/50), acc = 1.00, loss = 0.01 (14.6 examples/sec; 0.343 sec/batch), lr: 0.007738
2016-08-28 04:18:47.793998: step 8890/9000 (epoch 50/50), acc = 1.00, loss = 0.01 (14.6 examples/sec; 0.343 sec/batch), lr: 0.007738
2016-08-28 04:18:55.446077: step 8900/9000 (epoch 50/50), acc = 1.00, loss = 0.01 (12.9 examples/sec; 0.389 sec/batch), lr: 0.007738

Step 8900: train_loss = 0.020782, train_accuracy = 1.000
Step 8900: dev_loss = 0.372737, dev_accuracy = 0.840

2016-08-28 04:19:18.750884: step 8910/9000 (epoch 50/50), acc = 1.00, loss = 0.03 (15.5 examples/sec; 0.323 sec/batch), lr: 0.007738
2016-08-28 04:19:26.222135: step 8920/9000 (epoch 50/50), acc = 1.00, loss = 0.03 (14.9 examples/sec; 0.335 sec/batch), lr: 0.007738
2016-08-28 04:19:34.210336: step 8930/9000 (epoch 50/50), acc = 1.00, loss = 0.02 (13.0 examples/sec; 0.384 sec/batch), lr: 0.007738
2016-08-28 04:19:42.164072: step 8940/9000 (epoch 50/50), acc = 1.00, loss = 0.00 (13.5 examples/sec; 0.369 sec/batch), lr: 0.007738
2016-08-28 04:19:50.099717: step 8950/9000 (epoch 50/50), acc = 1.00, loss = 0.02 (14.2 examples/sec; 0.352 sec/batch), lr: 0.007738

Step 8950: train_loss = 0.020468, train_accuracy = 1.000
Step 8950: dev_loss = 0.368908, dev_accuracy = 0.830

2016-08-28 04:20:15.793906: step 8960/9000 (epoch 50/50), acc = 1.00, loss = 0.01 (12.3 examples/sec; 0.408 sec/batch), lr: 0.007738
2016-08-28 04:20:24.621645: step 8970/9000 (epoch 50/50), acc = 1.00, loss = 0.02 (10.9 examples/sec; 0.460 sec/batch), lr: 0.007738
2016-08-28 04:20:32.978843: step 8980/9000 (epoch 50/50), acc = 1.00, loss = 0.02 (13.9 examples/sec; 0.361 sec/batch), lr: 0.007738
2016-08-28 04:20:41.489364: step 8990/9000 (epoch 50/50), acc = 1.00, loss = 0.01 (13.7 examples/sec; 0.365 sec/batch), lr: 0.007738
2016-08-28 04:20:50.500041: step 9000/9000 (epoch 50/50), acc = 1.00, loss = 0.01 (14.5 examples/sec; 0.345 sec/batch), lr: 0.007738

Step 9000: train_loss = 0.021266, train_accuracy = 1.000
Step 9000: dev_loss = 0.362411, dev_accuracy = 0.830
