# python ./prepare.py
Skipped too short(< 3) or too long(> 100) sequences.
Sequence level: sentence
Saved 900+90 examples in "./data/tuba900/tuba.causal".
Saved 900+90 examples in "./data/tuba900/tuba.enable".
Saved 900+90 examples in "./data/tuba900/tuba.none".

# python ./util.py
17273 words found in training set. Truncated to vocabulary size 17000.
Dictionary saved to file /path/to/cnn-ld-tf/data/tuba900/vocab. Max sentence length in data is 96.
Split dataset into train/test set: 2700 for training, 270 for evaluation.

python ./train.py --batch_size=5 --num_classes=3 --vocab_size=17000 --sent_len=96 --use_pretrain=False \
--data_dir=./data/tuba900 --num_epoch=10 --optimizer='adagrad' --log_step=10 --summary_step=100 --checkpoint_step=100 \
--l2_reg=0.001 --dropout=0.6 --tolerance_step=100 --init_lr=0.01
